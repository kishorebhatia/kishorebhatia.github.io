<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>CloudBees Jenkins Platform</title><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"><link href="reveal.js/css/reveal.css" rel="stylesheet"><link rel="stylesheet" href="./build/build.css" id="theme"><link href="reveal.js/lib/css/zenburn.css" rel="stylesheet"><script>document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );</script></head><body><div class="reveal"><div class="slides"><section><h1>CloudBees Jenkins Platform</h1><div class="paragraph"><p>Pipeline with Docker</p></div><p><small></small></p></section>
<section id="toc"><h2>ToC</h2><div class="ulist"><ul><li><p><a href="#prerequisites">Prerequisites</a></p></li><li><p><a href="#jenkins_overview">Jenkins Overview</a></p></li><li><p><a href="#cloudbees_introduction">CloudBees Introduction</a></p></li><li><p><a href="#pipeline_introduction_day_1">Pipeline Introduction</a></p></li><li><p><a href="#docker_introduction_day_2">Docker Introduction</a></p></li><li><p><a href="#the_project_part_1_day_3">The Project (Part 1/2)</a></p></li><li><p><a href="#the_project_part_2_day_4">The Project (Part 2/2)</a></p></li></ul></div></section>
<section><section id="prerequisites"><h2>Prerequisites</h2></section><section id="prerequisites_2"><h2>Prerequisites</h2><div class="ulist"><ul><li><p>If you do not already have an SSH client, please install <a href="https://chrome.google.com/webstore/detail/secure-shell/pnhechapfaindjhompbnflcldabbghjo/related?hl=en">Secure Shell Chrome Extension</a></p></li><li><p>Confirm that you can login to the assigned server (IP has been sent by email). Use <strong>cb</strong> as both the user and the password.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>If you do not have an SSH client installed, please add the Secure <a href="https://chrome.google.com/webstore/detail/secure-shell/pnhechapfaindjhompbnflcldabbghjo">Shell Chrome Extension</a></p></div>
<div class="paragraph"><p>Check that the connection to the assigned server works correctly (please replace <strong>&lt;IP&gt;</strong> with the IP you were assigned).</p></div>
<div class="paragraph"><p>From SSH client: <code>ssh cb@&lt;IP&gt;</code></p></div>
<div class="paragraph"><p>From Secure Shell Chrome Extension: Type cb as user, <strong>&lt;IP&gt;</strong> as address, and <strong>22</strong> as port. Click the Connect button.</p></div>
<div class="paragraph"><p>The password is <strong>cb</strong>.</p></div></aside></section></section>
<section><section id="jenkins_overview"><h2>Jenkins overview</h2></section><section id="meet_jenkins"><h2>Meet Jenkins&#8230;&#8203;</h2><div class="ulist"><ul><li><p>#1 Continuous Integration and Delivery server</p></li><li><p>Created by Kohsuke Kawaguchi</p></li><li><p>An independent and active community (jenkins-ci.org)</p></li><li><p>10 years old</p></li><li><p>500+ releases to date</p></li><li><p>100,000 active installations</p></li><li><p>300,000 Jenkins servers</p></li><li><p>1,200+ plugins</p></li></ul></div>
<div class="imageblock" style=""><div class="content"><img src="./images/butler-w-kk.png" alt="butler w kk" width="200"></div></div>
<div class="paragraph footer"><p>Source: 2014 Java Tools and Technologies Landscape – Rebel Labs</p></div></section><section><div class="imageblock" style=""><div class="content"><img src="./images/java-leaderboard.png" alt="java leaderboard" width="800"></div></div></section><section id="jenkins_is_the_cd_orchestrator"><h2>Jenkins is the CD Orchestrator</h2><div class="imageblock" style=""><div class="content"><img src="./images/jenkins-cd-orchestrator.png" alt="jenkins cd orchestrator" width="600"></div></div></section></section>
<section><section id="cloudbees_introduction"><h2>CloudBees Introduction</h2></section><section id="cloudbees_and_the_jenkins_community"><h2>CloudBees and the Jenkins Community</h2><div class="imageblock" style="float: right"><div class="content"><img src="./images/butler-w-kk.png" alt="butler w kk" width="200"></div></div>
<div class="ulist"><ul><li><p>Kohsuke Kawaguchi</p><div class="ulist"><ul><li><p>Community leader and CloudBees’ CTO</p></li></ul></div></li><li><p>Code and Releases</p><div class="ulist"><ul><li><p>CloudBees partners with the community on development</p></li><li><p>CloudBees engineers contribute a majority of Jenkins OSS code</p></li><li><p>CloudBees partners with the community on releases</p></li><li><p>CloudBees contributes fixes back to the community</p></li></ul></div></li><li><p>Produce Jenkins Quarterly Newsletter</p></li><li><p>Conduct Jenkins User Conferences</p></li></ul></div></section><section id="cloudbees_products"><h2>CloudBees Products</h2><div class="imageblock" style=""><div class="content"><img src="./images/cloudbees-products.png" alt="cloudbees products" width="800"></div></div></section><section id="cloudbees_jenkins_platform_enterprise_edition"><h2>CloudBees Jenkins Platform: Enterprise Edition</h2><div class="paragraph"><p>For enterprise DevOps or infrastructure teams that want to:</p></div>
<div class="ulist"><ul><li><p>Have the best in class Jenkins technical support</p></li><li><p>Operate Jenkins at scale to support a large number of developers, teams, or projects</p></li><li><p>Monitor usage and share resources across the Jenkins infrastructure</p></li><li><p>Optimize performance and efficiency of the CD platform</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><a href="https://www.cloudbees.com/products/cloudbees-jenkins-platform/enterprise-edition" class="bare">https://www.cloudbees.com/products/cloudbees-jenkins-platform/enterprise-edition</a></p></div></aside></section><section id="cloudbees_subscription_model"><h2>CloudBees Subscription Model</h2><div class="ulist"><ul><li><p>CloudBees sells subscriptions that entitle you to receive support for:</p><div class="ulist"><ul><li><p>CloudBees Jenkins Platform</p></li><li><p>OSS Jenkins including 1,200+ Open Source Plugins</p></li></ul></div></li><li><p>Customers are charged an annual subscription fee per installation</p><div class="ulist"><ul><li><p>Multi-year subscriptions are available</p></li></ul></div></li><li><p>A typical service subscription includes:</p><div class="ulist"><ul><li><p>Software updates, bug fixes, and upgrades</p></li><li><p>Technical support</p></li><li><p>stable versions, stable APIs, and more</p></li></ul></div></li></ul></div></section><section id="cloudbees_resources"><h2>CloudBees Resources</h2><div class="ulist"><ul><li><p>Customer Engagement</p><div class="ulist"><ul><li><p>Support</p></li><li><p>Knowledge Base</p></li><li><p>Diagnostics</p></li></ul></div></li><li><p>Professional Services</p><div class="ulist"><ul><li><p>Architecture Assessment &amp; CD Guidance</p></li><li><p>Bootstrap Implementation Services</p></li><li><p>Migration Assistance</p></li></ul></div></li><li><p>Partners</p><div class="ulist"><ul><li><p>Training</p></li><li><p>Integration</p></li><li><p>Custom Development</p></li></ul></div></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Support &amp; Knowledge Base
<a href="https://cloudbees.zendesk.com/hc/en-us" class="bare">https://cloudbees.zendesk.com/hc/en-us</a></p></div>
<div class="paragraph"><p>Diagnostics
<a href="https://cloudbees.zendesk.com/hc/en-us/articles/203805390" class="bare">https://cloudbees.zendesk.com/hc/en-us/articles/203805390</a></p></div>
<div class="paragraph"><p>Professional Services
<a href="https://www.cloudbees.com/products/professional-services" class="bare">https://www.cloudbees.com/products/professional-services</a></p></div></aside></section></section>
<section><section id="pipeline_introduction_day_1"><h2>Pipeline Introduction (Day 1)</h2><aside class="notes"><div class="paragraph"><p>The aim of the first day of the workshop is to introduce Pipeline on theoretical level (without hands-on). It will provide a short history of other plugins that led to development of the Pipeline, provide reasons for its existence, and an introduction to its syntax.</p></div></aside></section><section id="in_this_unit_you_will_learn"><h2>In This Unit: You Will Learn</h2><div class="ulist"><ul><li><p>The need for pipeline</p></li><li><p>Pipeline use cases</p></li><li><p>Key pipeline DSL</p></li><li><p>Pipeline structure and syntax</p></li><li><p>Execution control</p></li></ul></div></section><section id="in_this_unit_you_will_be_able_to"><h2>In This Unit: You Will Be Able To</h2><div class="ulist"><ul><li><p>Recognize pipeline purpose and use cases</p></li></ul></div></section><section id="the_need_for_pipeline"><h2>The Need For Pipeline?</h2><div class="ulist"><ul><li><p>Reduction of a number of jobs</p></li><li><p>Easier maintenance</p></li><li><p>Deployment decentralization</p></li><li><p>Easier specification through code</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Reduction of a number of jobs:</strong></p></div>
<div class="paragraph"><p>When there are tens of jobs, their maintenance is quite easy. However, when that number increases to hundreds, managing them can become quite tedious and time demanding.</p></div>
<div class="paragraph"><p>Let’s say that an average CD flow has five jobs (building, pre-deployment testing, deployment to a staging environment, post-deployment testing, and deployment to production). In reality, there are often more than five jobs but let’s keep it an optimistic estimate. If we multiple those jobs with, let’s say, twenty flows belonging to twenty different projects, the total number reaches one hundred.</p></div>
<div class="paragraph"><p><strong>Easier maintenance:</strong></p></div>
<div class="paragraph"><p>Now, imagine that we need to change all those jobs from, let’s say, Maven to Gradle. We can choose to start modifying them through the Jenkins UI or be brave and apply changes directly in Jenkins XML files that represent those jobs. Either way, this, seemingly simple change, would require quite some dedication. Fortunately, CloudBees Jenkins Enterprise Platform has templating functionality that simplifies massive modifications to multiple jobs.</p></div>
<div class="paragraph"><p><strong>Deployment decentralisation:</strong></p></div>
<div class="paragraph"><p>Even with it, maintaining many jobs chained into a flow is a complex endeavour. Moreover, due to its nature, everything is centralized in one location making it hard for teams to manage jobs belonging to their own projects.</p></div>
<div class="paragraph"><p><strong>Easier specification through code:</strong></p></div>
<div class="paragraph"><p>Some time ago, it become fashionable to use UIs for everything. We got a lot of tools that relied almost completely on drag&amp;drop operations. Since then, we learned that there there should be a healthy division between the usage of UIs and code. Deployment flow is one of those cases when something is easier to express, and cheaper to maintain, when specified as code. Traditional Jenkins jobs proved to introduce unnecessary problems when dealing with complex flows, conditionals, and so on.</p></div></aside></section><section id="what_is_cd_flow"><h2>What is CD Flow?</h2><div class="imageblock" style=""><div class="content"><img src="./images/CD-pipeline.png" alt="CD pipeline" width="500"></div></div>
<div class="paragraph"><p>CD is about setting a "flow", from SCM commit to deployed application</p></div>
<div class="paragraph"><p>Jenkins can chain jobs this way, but &#8230;&#8203;</p></div>
<aside class="notes"><div class="paragraph"><p>The main requirement of continuous delivery (CD) is to set a "flow" that will run the whole process, starting from SCM commit, through building and testing, all the way until deployment. Jenkins has been capable of creating such a flow almost from the day it was born. However, being capable of doing something does not necessarily mean that something is done efficiently and easy to maintain. Jenkins had quite a lot of deficiencies preventing us from exploiting CD flow to its full potential. We can chain jobs but&#8230;&#8203;</p></div></aside></section><section id="jenkins_cd_flow"><h2>Jenkins CD Flow</h2><div class="paragraph"><p><strong>A Real-world CD Flow Is Way More Complex !</strong></p></div>
<div class="ulist"><ul><li><p>Requires (Complex) Conditional Logic</p></li><li><p>Requires Resources Allocation And Cleanup</p></li><li><p>Involves Human Interaction For Manual Approval</p></li><li><p>Should Be Resumable At Some Point On Failure</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>A real-world CD flow is often much more complex than what chained jobs can offer. Even when we can accomplish the objectives, the result is cumbersome, hard to maintain, and difficult to visualise. Some of the CD objectives we are unable to solve through chained jobs or are proved to be difficult to implement, are as follows.</p></div>
<div class="paragraph"><p><strong>Requires (complex) conditional logic:</strong></p></div>
<div class="paragraph"><p>CD flows often require some conditional logic to be applied. In many cases, it is not enough to simply chain jobs in a linear fashion. Often, we do not want to simply say run job A, once it&#8217;s finished, run job B, and follow it with job C. In real-world situations, things are more complicated than that. We want to run something (call it A), depending on the result invoke B1 or B2, than run in parallel C1, C2, and C3, and, finally, execute job D only when C jobs are finished successfully. If this would be a program or a script, we would have no problem accomplishing something like that since all modern programming languages allow us to employ conditional logic in a simple and efficient way.</p></div>
<div class="paragraph"><p><strong>Requires resources allocation and cleanup:</strong></p></div>
<div class="paragraph"><p>Resource allocation needs a careful thought and, often, is more complicated than a simple decision to run a job on a predefined slave. There are cases when slave should be decided dynamically, workspace should be defined during runtime, and cleanup depends on a result of some action.</p></div>
<div class="paragraph"><p><strong>Involves human interaction for manual approval:</strong></p></div>
<div class="paragraph"><p>While continuous deployment process means that the whole flow ends with deployment to production, many business are not ready for such a scope, or have use cases when it is not appropriate. Any other process with a smaller scope, be it continuous delivery or continuous integration, often requires some human interaction. A step in the flow might need someone&#8217;s confirmation, failed process might need a manual input about reasons for the failure, and so on. Requirement for human interaction should be an integral part of the flow and should allow us to pause, inspect, and resume the flow.</p></div>
<div class="paragraph"><p><strong>Should be resumable at some point on failure:</strong></p></div>
<div class="paragraph"><p>In many cases, it can take a long time for deployment flows to be fully executed. It is not uncommon for them to run for a couple of hours, or even days. In such cases, failure of the process, or the whole node the process is running on, should not mean that everything should be repeated. We should have a mechanism to continue the flow from defined checkpoints thus avoiding costly repetition, potential delays, and additional costs.</p></div></aside></section><section id="jenkins_cd_flow_2"><h2>Jenkins CD Flow</h2><div class="ulist"><ul><li><p>Many Related Plugins</p><div class="ulist"><ul><li><p>Conditional build steps</p></li><li><p>Parameterized trigger</p></li><li><p>Promotions</p></li><li><p>&#8230;&#8203;</p></li></ul></div></li><li><p>Major Issue: Flow Configuration Is Scattered In Various Jobs</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Jenkins already had plugins that could help us designing and building CD flows. We got "Conditional build steps", "Parameterised trigger", "Promotions", and so on. One of the major issues with those plugins is configuration. It tends to be scattered across multiple locations, hard to maintain, and with low visibility.</p></div></aside></section><section id="cd_flow_visualization"><h2>CD Flow Visualization</h2><div class="paragraph"><p>Build Pipeline Plugin To Render “Linear” Pipeline</p></div>
<div class="imageblock" style=""><div class="content"><img src="./images/pipeline-old-1.png" alt="pipeline old 1" width="700"></div></div>
<div class="paragraph"><p>Build-graph-view To Render Complex Ones</p></div>
<div class="imageblock" style=""><div class="content"><img src="./images/pipeline-old-2.png" alt="pipeline old 2" width="700"></div></div>
<aside class="notes"><div class="paragraph"><p>The "Build Pipeline" plugin helps us in visualisation of the process but failed to address any other need of the CD flow.</p></div></aside></section><section id="reality_check"><h2>Reality Check</h2><div class="ulist"><ul><li><p>How do you use Jenkins now?</p></li><li><p>Given the functionality, where do you see your team using these tools and plugins in your jobs?</p></li><li><p>Do you practice CI or CD?</p></li><li><p>Describe your CI/CD flow needs and processes.</p></li></ul></div></section><section id="build_flow_plugin"><h2>Build Flow Plugin</h2><div class="ulist"><ul><li><p>OSS</p></li><li><p>Proof of concept</p></li><li><p>Define CD flow as code</p></li><li><p>Largely adopted</p></li><li><p>Technical limitations</p></li></ul></div>
<div class="imageblock" style=""><div class="content"><img src="./images/build-flow.png" alt="build flow" width="300"></div></div>
<aside class="notes"><div class="paragraph"><p><strong>OSS:</strong></p></div>
<div class="paragraph"><p>During mid. 2012, the first beta version of the Build Flow Plugin has been released as open source.</p></div>
<div class="paragraph"><p><strong>Proof of concept:</strong></p></div>
<div class="paragraph"><p>More than a final solution to CD flow problems, it was considered a proof of concept.</p></div>
<div class="paragraph"><p><strong>Define CD flow as code:</strong></p></div>
<div class="paragraph"><p>The main idea behind the plugin was to define the CD flow through code. To simplify coding, Groovy based DSL was created.</p></div>
<div class="paragraph"><p><strong>Largely adopted:</strong></p></div>
<div class="paragraph"><p>The response from the community was very positive and the plugin received a wide adoption that demonstrated that the direction of the plugin was correct and should be explored more.</p></div>
<div class="paragraph"><p><strong>Technical limitations:</strong></p></div>
<div class="paragraph"><p>The plugin hit some technical limitations that prevented the community from developing it further. We had to start fresh using the knowledge and experience obtained through the Build Flow plugin.</p></div></aside></section><section id="cloudbees_pipeline_plugin"><h2>CloudBees Pipeline Plugin</h2><div class="ulist"><ul><li><p>Inspired By Build-flow</p></li><li><p>Groovy DSL To Orchestrate Build Steps</p></li><li><p>Extensible DSL Syntax</p></li><li><p>Supports Checkpoint</p></li><li><p>Advanced Visualization</p></li><li><p>SCM Friendly</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Inspired by build-flow:</strong></p></div>
<div class="paragraph"><p>The success of the Build Flow plugin lead us into a decision to start fresh. The Workflow plugin was born (and later renamed to Pipeline plugin). It continues maintaining the core idea of the Build Flow plugin and the experience obtained while developing it allowed us to avoid some of the mistakes and greatly improve the design.</p></div>
<div class="paragraph"><p><strong>Groovy DSL to orchestrate build steps:</strong></p></div>
<div class="paragraph"><p>The Pipeline plugin is based on Groovy DSL that can be used to define build steps and accomplish in a single script flow that would normally require many "standard" Jenkins chained together. The DSL Groovy based syntax allows us to combine the best of both worlds. Through DSL we have simplicity in defining commonly used tasks like access to a SCM repository, definition of nodes tasks should run on, parallel execution, and so on. On the other hand, since the plugin uses Groovy, almost any operation can be defined. We can, finally, use conditionals, loops, variables, and so on. Since Groovy is an integral part of Jenkins, we can also use it to access almost any existing Jenkins plugin or even to operate on Jenkins core features. If you are not familiar with Groovy (and do not want to get to know it), in most cases you will be able to accomplish everything through the DSL. You can consider it a new language with a very simple syntax. Domain specific languages have been around for a long time and proved to be more efficient at defining very specific sets of tasks.</p></div>
<div class="paragraph"><p><strong>Extensible DSL syntax:</strong></p></div>
<div class="paragraph"><p>The plugin is designed in a way that it can be easily extended. Even though it&#8217;s been in use only for a short time, we&#8217;ve seen many contributions. While we can use Groovy to access any plugin, long term plan is to extend the DSL so that all commonly used plugins are defined through it.</p></div>
<div class="paragraph"><p><strong>Supports checkpoint:</strong></p></div>
<div class="paragraph"><p>Besides ever growing number of DSL modules, the Pipeline supports checkpoints allowing us to restart the flow in defined points. Bear in mind that they are one of the enterprise features not available in the open source version.</p></div>
<div class="paragraph"><p><strong>Advanced visualisation:</strong></p></div>
<div class="paragraph"><p>The idea, from the start, was to combine the flexibility and maintainability accomplished with DSL and Groovy with advanced visualisation. We define the flow through code and monitor the result through visualisation. Just as checkpoints, it is one of the features available only in the enterprise version.</p></div>
<div class="paragraph"><p><strong>SCM friendly:</strong></p></div>
<div class="paragraph"><p>Since the whole delivery flow is defined as code in plain text format, storing the scripts in SCM is not only available, but highly recommended. By storing the Pipeline scripts in, let&#8217;s say, Git, we can apply the same process with them as with any other code. We can commit it to the repository, use pull requests, code reviews, and so on. Moreover, the Multibranch Pipeline allows us to store the script in a Jenkinsfile and defined different flows inside each branch.</p></div>
<div class="paragraph"><p>This was only a sneak peak into the Pipeline plugin capabilities. We&#8217;ll discuss its syntax, capabilities, and features in more detail later on as we progress with the training. At this point you can already see that it comes with many advantages over more "traditional" ways of defining jobs. Indeed, the Pipeline plugin opened some doors that were previously closed or very hard to pass through. It brought Jenkins to a completely new level proving, once again, that it is the leader among CI/CD tools.</p></div></aside></section><section id="mid_break"><h2>Mid-Break</h2><div class="paragraph"><p>(10) minutes for learner re-integration.</p></div>
<div class="imageblock" style=""><div class="content"><img src="./images/break.png" alt="break" width="300"></div></div></section><section id="creating_a_simple_pipeline"><h2>Creating A Simple Pipeline</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-create-pipeline.png" alt="cje create pipeline" width="700"></div></div>
<div class="paragraph"><p>More info: <a href="https://jenkins.io/doc/pipeline/">Getting Started With Pipeline</a></p></div>
<aside class="notes"><div class="paragraph"><p>Creating a new Pipeline job is easy and follows the same process like any other Jenkins job. Shall we get our hands dirty? Let&#8217;s create a new Pipeline job.</p></div>
<div class="paragraph"><p>Please open the Jenkins instance running on the 10.100.192.200:8080 address. Click on the New Item click located in the left-hand menu, type my-first-pipeline, select Pipeline from the the of available job types, and, finally, click the OK button. You will be redirected to the my-first-pipeline configuration screen. You will notice that there are no options to add build steps or post-build actions. Apart from some common options like build parameters and the ability to run the job periodically, the whole job consists of a single Pipeline definition. A simple workflow script could be as follows.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">node("cd") {
    echo "This is a message from Jenkins Pipeline plugin"
}</code></pre></div></div>
<div class="paragraph"><p>Once you are done writing the script, please click the Save button and build it by pressing the Build Now link located in the left-hand menu. Since our job is only outputting a message, it will take only a second before it finishes executing. Click on the build number #1, followed with the Console Output. You&#8217;ll notice two things. The build was run on the node-cd slave. We accomplished that by specifying one of its labels cd inside the script and wrapped everything else inside curly braces. Once Jenkins got connected to the slave, it output "This is a message from Jenkins Workflow plugin" to the screen.</p></div>
<div class="paragraph"><p>With time, you will learn Workflow DSL syntax. Until that happens, it would be nice to have a way to explore modules we can use without consulting the documentation.</p></div></aside></section><section id="snippet_generator"><h2>Snippet Generator</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-pipeline-snippet-generator.png" alt="cje pipeline snippet generator" width="600"></div></div>
<aside class="notes"><div class="paragraph"><p>At this moment you might be worried that you&#8217;ll have to learn yet another language. While that is true on the long run, you will be able to start creating Pipeline jobs right away by leveraging the Snippet Generator. It is an easy and convenient way to explore available Pipeline modules and generate snippets that can be copied into the Pipeline script.</p></div>
<div class="paragraph"><p>Please open the my-first-workflow job configuration screen. Scroll down to the Pipeline script and click the Snippet Generator checkbox. You&#8217;ll see the Sample Step drop-down with the list of all available steps that can be used. Choose one. Fields with arguments that can be used with the selected step will appear below. Fill them in and click the Generate Groovy button. A snippet will appear below the button. The only thing left is to copy it and paste it into appropriate part of the Pipeline Script.</p></div>
<div class="paragraph"><p>Later on, we&#8217;ll go into more detailed discussion about some of the most commonly used Pipeline steps.</p></div></aside></section><section id="pipeline_introduction_review"><h2>Pipeline Introduction: Review</h2></section><section id="pipeline_introduction_review_2"><h2>Pipeline Introduction: Review</h2><div class="ulist"><ul><li><p>CD Flow</p></li><li><p>Older Solutions</p></li><li><p>Build Flow Plugin</p></li><li><p>Cloudbees Pipeline Plugin</p></li><li><p>Create Pipeline Jobs</p></li><li><p>Use The Snippet Generator</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>CD Flow:</strong></p></div>
<div class="paragraph"><p>We briefly explored the goals of a CD flow as a definition of a flow that defined the whole process, starting from SCM commit, through building and testing, all the way until deployment.</p></div>
<div class="paragraph"><p><strong>Older solutions:</strong></p></div>
<div class="paragraph"><p>We continued discussing some of the older Jenkins solutions to this challenge and concluded that a real-world CD flow is way more complex than what those plugins can offer. The flow often requires complex and conditional logic, allocation and cleanup of resources, can involve human interaction, should be resumable in case of a failure, and so on. We also discussed the need to a balanced combination of code and visualisation of the pipeline.</p></div>
<div class="paragraph"><p><strong>Build Flow plugin:</strong></p></div>
<div class="paragraph"><p>We could not begin the discussion of the CloudBees Pipeline plugin without mentioning the Build Flow plugin. It is, in a way, the predecessor of the CloudBees Pipeline plugin and uses similar concepts (Groovy, DSL, and so on). Even though it got wide adoption, it was considered only a proof of concept.</p></div>
<div class="paragraph"><p><strong>CloudBees Pipeline Plugin:</strong></p></div>
<div class="paragraph"><p>CloudBees Pipeline plugin is an evolution of the CD concepts inside Jenkins and a response to ever changing needs of the industry. We went briefly through its major advantages and reasons behind its existence.</p></div>
<div class="paragraph"><p><strong>Create Pipeline jobs:</strong></p></div>
<div class="paragraph"><p>We saw, briefly, how to create a new Pipeline job by implementing a simple echo running inside a slave with the cd label. The intention behind the example was only to give you a taste of what we&#8217;ll explore in more details through the rest of the training.</p></div>
<div class="paragraph"><p><strong>Use the Snippet Generator:</strong></p></div>
<div class="paragraph"><p>We used the Snippet Generator to help us create snippets of the Pipeline steps. It is a convenient way to explore the Pipeline DSL without having to memorise the syntax.</p></div></aside></section><section id="pipeline_introduction_exercise"><h2>Pipeline Introduction: Exercise</h2><div class="paragraph"><p><a href="labs.html#_pipeline_introduction_exercise">Pipeline Introduction: Exercise</a></p></div></section></section>
<section><section id="docker_introduction_day_2"><h2>Docker Introduction (Day 2)</h2><aside class="notes"><div class="paragraph"><p>Today we&#8217;ll take a brake from the CloudBees Pipeline plugin and explore Docker and the tools around it.</p></div></aside></section><section id="review_of_day_1_concepts_and_exercise"><h2>Review Of Day 1 Concepts And Exercise</h2><div class="ulist"><ul><li><p>CD flow</p></li><li><p>Older solutions</p></li><li><p>Build Flow plugin</p></li><li><p>CloudBees Pipeline Plugin</p></li><li><p>Create Pipeline jobs &amp; use the Snippet Generator</p></li><li><p>Exercise</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>CD flow:</strong></p></div>
<div class="paragraph"><p>We briefly explored the goals of a CD flow as a definition of the whole process, starting from SCM commit, through building and testing, all the way until deployment.</p></div>
<div class="paragraph"><p><strong>Older solutions:</strong></p></div>
<div class="paragraph"><p>We continued discussing some of the older Jenkins solutions to this challenge and concluded that a real-world CD flow is way more complex than what those plugins can offer. The flow often requires complex and conditional logic, allocation and cleanup of resources, can involve human interaction, should be resumable in case of a failure, and so on. We also discussed the need to a balanced combination of code and visualisation of the flow.</p></div>
<div class="paragraph"><p><strong>Build Flow plugin:</strong></p></div>
<div class="paragraph"><p>We could not begin the discussion of the CloudBees Pipeline plugin without mentioning the Build Flow plugin. It is, in a way, the predecessor of the CloudBees Pipeline plugin and uses similar concepts (Groovy, DSL, and so on). Even though it got wide adoption, it was considered only a proof of concept.</p></div>
<div class="paragraph"><p><strong>CloudBees Pipeline Plugin:</strong></p></div>
<div class="paragraph"><p>CloudBees Pipeline plugin is an evolution of the CD concepts inside Jenkins and a response to ever changing needs of the industry. We went briefly through its major advantages and reasons behind its existence.</p></div>
<div class="paragraph"><p><strong>Create Pipeline jobs:</strong></p></div>
<div class="paragraph"><p>We saw, briefly, how to create a new Pipeline job by implementing a simple echo running inside a slave with the cd label. The intention behind the example was only to give you a taste of what we&#8217;ll explore in more details through the rest of the training.</p></div>
<div class="paragraph"><p><strong>Use the Snippet Generator:</strong></p></div>
<div class="paragraph"><p>We used the Snippet Generator to help us create snippets of the Pipeline steps. It is a convenient way to explore the Pipeline DSL without having to memorise the syntax.</p></div>
<div class="paragraph"><p><strong>Exercise:</strong></p></div>
<div class="paragraph"><p><em>TEACHER DISPLAYS AND COMMENTS ON A FEW OF THE RESULTS PARTICIPANTS SENT</em></p></div></aside></section><section id="in_this_unit_you_will_learn_2"><h2>In This Unit: You Will Learn</h2><div class="ulist"><ul><li><p>How to prepare the environments</p></li><li><p>Docker containers</p></li><li><p>Docker use cases</p></li><li><p>Docker tools</p></li></ul></div></section><section id="in_this_unit_you_will_be_able_to_2"><h2>In This Unit: You Will Be Able To</h2><div class="ulist"><ul><li><p>Prepare the CD flow environments</p></li><li><p>Understand Docker containers and use cases</p></li><li><p>Have A Working Knowledge Of Docker Tools</p></li></ul></div></section><section id="docker_containers"><h2>Docker Containers</h2><div class="ulist"><ul><li><p>Self-sufficient</p></li><li><p>Isolated</p></li><li><p>Immutable</p></li><li><p>Reliable</p></li><li><p>Scalable</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Self-sufficient:</strong></p></div>
<div class="paragraph"><p>Traditional deployments would put an artifact into an existing node expecting that everything else is in place; the application server, configuration files, dependencies, and so on. Containers, on the other hand, contain everything our software needs. Each container should be self-sufficient. It should contain everything an application needs. That includes, but is not limited to, run-time dependencies, built artefacts (JARs, WARs, DLLs), application server, and so on.</p></div>
<div class="paragraph"><p><strong>Isolated:</strong></p></div>
<div class="paragraph"><p>Each container runs in an isolated process thus making it more fault-tollerant, easier to deploy and more secure.</p></div>
<div class="paragraph"><p><strong>Immutable:</strong></p></div>
<div class="paragraph"><p>Containers consist of a number of unchangeable images making them immutable thus avoiding differences that might between different environments. Applications packed inside containers behave exactly the same no matter whether they are deployed to developer laptop, staging server, or production.</p></div>
<div class="paragraph"><p><strong>Reliable:</strong></p></div>
<div class="paragraph"><p>When isolation, self-sufficiency, and immutability are combined, the result is higher reliability.</p></div>
<div class="paragraph"><p><strong>Scalable:</strong></p></div>
<div class="paragraph"><p>Since each container is immutable, scaling an application is much easier than when other packaging and deployment methods are used.</p></div></aside></section><section id="vms_vs_docker_containers"><h2>VMs vs Docker Containers</h2><div class="imageblock" style=""><div class="content"><img src="./images/vm-vs-docker.png" alt="vm vs docker" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>A physical server running, for example, five virtual machines would have five operating systems in addition to a hypervisor that is more resource demanding than lxc. Five containers, on the other hand, share the operating system of the physical server and, where appropriate, binaries and libraries. As a result, containers are much more lightweight then VMs. That is not such a big difference with monolithic applications especially in cases when a single one would occupy the whole server. With microservices, this gain in resource utilization is very important considering that we might have tens or hundreds of them on a single physical server. Put in other words, a single physical server can host more containers than virtual machines.</p></div></aside></section><section id="single_container_service"><h2>Single-Container Service</h2><div class="imageblock" style=""><div class="content"><img src="./images/container-single-service.png" alt="container single service" width="500"></div></div>
<aside class="notes"><div class="paragraph"><p>Containers are self-sufficient bundles that contain everything we need (with the exception of the kernel), run in an isolated process and are immutable. Being self-sufficient means that a container commonly has the following components.</p></div>
<div class="ulist"><ul><li><p>Runtime libraries (JDK, Python, or any other library required for the application to run)</p></li><li><p>Application server (Tomcat, nginx, and so on)</p></li><li><p>Database (preferably lightweight)</p></li><li><p>Artifacts (JAR, WAR, static files, and so on)</p></li></ul></div></aside></section><section id="linked_or_networked_containers"><h2>Linked or Networked Containers</h2><div class="imageblock" style=""><div class="content"><img src="./images/container-links.png" alt="container links" width="700"></div></div>
<aside class="notes"><div class="paragraph"><p>Fully self-sufficient containers are the easiest way to deploy services but pose a few problems with scaling. If we&#8217;d like to scale such a container on multiple nodes in a cluster, we&#8217;d need to make sure that databases embedded into those containers are synchronized or that their data volumes are located on a shared drive. The first option often introduces unnecessary complexity while shared volumes might have a negative impact on performance. Alternative is to make containers almost self-sufficient by externalizing database into a separate container. In such a setting there would be two different containers per each service. One for the application and the other for the database. They would be linked (preferably through a proxy service). While such a combination slightly increases deployment complexity, it provides greater freedom when scaling. We can deploy multiple instances of the application container or several instances of the database depending performance testing results or increase in traffic. Finally, nothing prevents us to scale both if such a need arises.</p></div></aside></section><section id="docker_use_cases"><h2>Docker Use Cases</h2><div class="ulist"><ul><li><p>Local Development And Testing</p></li><li><p>Continuous Integration, Delivery, Or Deployment</p></li><li><p>Reliable And Repeatable Deployments</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Local development and testing:</strong></p></div>
<div class="paragraph"><p>Due to containers self-sufficiency and immutability, setting up local environment poses no effort, and the result is exactly the same no matter whether they are run on laptops or servers. Running a full application requires a single command through Docker CLI. A whole system can be defined in a single Docker Compose configuration reducing deployment of any number of services a breeze.</p></div>
<div class="paragraph"><p><strong>Continuous integration, delivery, or deployment:</strong></p></div>
<div class="paragraph"><p>Since there is no need to provision environments (except Docker engine itself), CI/CD is much simplified especially setting up nodes where testing and building will be performed. Moreover, containers can be removed after they run so that no trace of them is left of the server. We can, for example, run a container with tests and remove it at the end. That would be equivalent to creating a new slave VM for each tests run and removing it at the end thus freeing resources for other tasks. The major difference is that running containers is much faster than creating and initialising VMs.</p></div>
<div class="paragraph"><p><strong>Reliable and repeatable deployments:</strong></p></div>
<div class="paragraph"><p>Having a guarantee that a container will behave in the same way no matter the environment, deployments are more reliable since we know that the application tested in, for example, staging environment, will behave the same when deployed to production. We can use containers to deploy applications from development, through staging, all the way until production environments.</p></div></aside></section><section id="reality_check_2"><h2>Reality Check</h2><div class="ulist"><ul><li><p>Questions About Containers?</p></li><li><p>Examples Of Scalability In Your Current Use Of Containers?</p></li><li><p>How Is CI Valuable In Your Environments?</p></li><li><p>Are You Using Containers In Deployment Now?</p></li></ul></div></section><section id="docker_tools"><h2>Docker Tools</h2><div class="ulist"><ul><li><p>Docker Hub/Registry</p></li><li><p>Docker Engine</p></li><li><p>Docker Compose</p></li><li><p>Docker Swarm</p></li><li><p>Docker Machine</p></li><li><p>Kitematic</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>A very big ecosystem has been created around Docker. New operating systems specialised in running containers have emerged. Two examples would be CoreOS and Rancher. We got both commercial and open source solutions to store containers, different toolkits to define containers, clustering and monitoring tools, software defined networking solutions, network storage, and so on. Today we&#8217;ll focus on tools developed by Docker itself, namely, Docker Hub and Registry, Engine, Compose, Swarm, Machine and Kitematic.</p></div></aside></section><section id="docker_hub_and_registry"><h2>Docker Hub and Registry</h2><div class="imageblock right top" style=""><div class="content"><img src="./images/docker-hub.png" alt="docker hub" width="150"></div></div>
<div class="ulist"><ul><li><p><strong>Docker Hub</strong>: Cloud hosted service that provides registry capabilities for public and private content.</p></li><li><p><strong>Docker Registry</strong>: An open source application dedicated to the storage and distribution of your Docker images.</p></li><li><p><strong>Docker Trusted Registry</strong>: Allows storing and managing Docker images on-premise. It supports security or regulatory compliance requirements.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Docker Hub:</strong></p></div>
<div class="paragraph"><p>Docker Hub is a cloud hosted service from Docker that provides registry capabilities for public and private content. We can push our containers to it and pull it on any server that has the access to internet. Moreover, it is a very useful tool to find already pre-made containers. If you need MongoDB, NodeJS server, ElasticSearch, or almost any other remotely popular tool or application, chances are that you&#8217;ll find at least one version in Docker Hub. For example, both Jenkins OS and CloudBees Jenkins Enterprise edition container are available in Docker. We can run them with a single docker run command. As you&#8217;ll see, we&#8217;ll use an example service prepared exclusively for this training and freely available in Docker Hub. Please note that Docker Hub allows both public and private accounts.</p></div>
<div class="paragraph"><p><strong>Docker Registry:</strong></p></div>
<div class="paragraph"><p>Docker Registry is an open source application dedicated to the storage and distribution of your Docker images. Many companies do not want to store their containers in Docker Hub, and prefer using Docker Registry as an alternative. Functionally, it is very similar to Docker Hub. The major difference is that it can be hosted on-premise or in your private cloud. Bear in mind that, unlike Docker Hub, it does not have a UI and needs to be operated completely through its CLI.</p></div>
<div class="paragraph"><p><strong>Docker Trusted Registry:</strong></p></div>
<div class="paragraph"><p>Docker Trusted Registry allows you to store and manage your Docker images on-premise or in your virtual private cloud to support security or regulatory compliance requirements in keeping data and applications your infrastructure. The solution is based on Docker Registry with additional features mostly related to security. It also features and UI that can be used to operate and monitor containers.</p></div>
<div class="paragraph"><p>Shell we give it a try and see Docker Hub and Docker Registry in action?</p></div></aside></section><section id="docker_hub_and_registry_pulling_and_pushing_containers"><h2>Docker Hub and Registry: Pulling and Pushing Containers</h2><div class="paragraph"><p>Please use the SSH client to connect to the server you were assigned.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker pull cloudbees/training-books-ms-tests

docker tag cloudbees/training-books-ms-tests \
    localhost:5000/training-books-ms-tests

docker push localhost:5000/training-books-ms-tests</code></pre></div></div>
<aside class="notes"><div class="paragraph"><p>We&#8217;ll start by entering into the VM we prepared for this training. Please use the SSH client to connect to the server you were assigned.</p></div>
<div class="paragraph"><p>Pull the container from Docker Hub:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker pull cloudbees/training-books-ms-tests</code></pre></div></div>
<div class="paragraph"><p>Next, we&#8217;ll pull a sample service from Docker Hub. The container was pushed to the public account and is freely available for pulling without any credentials. In order to save some time and not risk with slow internet connection, the image we are about to pull has already been preloaded inside the VM. As a result, you&#8217;ll see Status: Image is up to date for cloudbees/training-books-ms-tests message. This shows another great Docker feature. Each container consists of multiple images. When you pull a container, only those images that have been modified will be downloaded thus saving time.</p></div>
<div class="paragraph"><p>Tag the container for private registry:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker tag -f cloudbees/training-books-ms-tests \
    localhost:5000/training-books-ms-tests</code></pre></div></div>
<div class="paragraph"><p>Now that we have the container pulled into the VM, we can see the other solution for storing and retrieving containers. We&#8217;ll push the image we pulled form Docker Hub into Registry running locally. Actually, we do not have any other option since I am not willing to share my Docker Hub credentials. Without them, you would not be able to push a new version to Docker Hub. Pushing to the private registry relies on the naming convention. If the name of the container is prefixed with the IP and the port of the private registry, when docker push is run, Docker Engine will send it to the specified address. In other words, pushing cloudbees/training-books-ms-tests would result in that container stored in Docker Hub. On the other hand, if we change the name of the container to localhost:5000/training-books-ms-tests, Docker Engine will store it in our private registry located in the specified IP and port. To accomplish this, we can use the tag command specifying the source container name followed by the new tag.</p></div>
<div class="paragraph"><p>Push the container to private registry:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker push localhost:5000/training-books-ms-tests</code></pre></div></div>
<div class="paragraph"><p>Now that we have our container properly tagged with our registry IP and port, we can push it. You&#8217;ll notice that Docker Engine listed quite a few images that form this container. The status of each of them is Image already exists. Again, we tried to save some time so pushing to the registry was part of the pre-load script we run as part of the preparation for this training. This was an useful demonstration that Docker Engine monitors each image separately and pushes only those that changed.</p></div>
<div class="paragraph"><p>The container can now be pulled from any server</p></div>
<div class="paragraph"><p>We mentioned Docker Engine a couple of times and used few of its commands in conjunction with Docker Hub and Private Registry for pulling, tagging, and pushing. Let&#8217;s discuss it in more detail.</p></div></aside></section><section id="docker_engine"><h2>Docker Engine</h2><div class="imageblock right top" style=""><div class="content"><img src="./images/docker-engine.png" alt="docker engine" width="250"></div></div>
<div class="ulist"><ul><li><p><strong>Docker Engine</strong> is a lightweight runtime and robust tooling that builds and runs Docker containers.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Docker Engine is a lightweight runtime and robust tooling that builds and runs Docker containers. Even though this was the official definition, Docker Engine does much more than that. It can run, pull, and push containers, create software defined network, manage volumes located on the host or network drives, and so on. Going through all the commands would require a much more time than what we have, so we&#8217;ll focus on a few most common examples and the syntax behind them. Please note that tomorrow we&#8217;ll combine Docker with Jenkins Pipeline. That will give us opportunity to explore both in more depth. Today, we&#8217;ll focus on Docker alone.</p></div></aside></section><section id="docker_engine_running_tests_inside_containers"><h2>Docker Engine: Running Tests Inside Containers</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">cd /mnt/training-books-ms

docker run -it --rm \
    -v $PWD/client/components:/source/client/components \
    -v $PWD/client/test:/source/client/test \
    -v $PWD/src:/source/src \
    -v $PWD/target/scala-2.10:/source/target/scala-2.10 \
    --env TEST_TYPE=all \
    localhost:5000/training-books-ms-tests

ll target/scala-2.10/

docker ps -a | grep books</code></pre></div></div>
<aside class="notes"><div class="paragraph"><p>Running Tests Inside Containers</p></div>
<div class="paragraph"><p>Enter the source code directory:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">cd ~/training-books-ms</code></pre></div></div>
<div class="paragraph"><p>Let&#8217;s enter the training-books-ms directory. It contains the source code of the container we already pulled from Docker Hub.</p></div>
<div class="paragraph"><p>Run tests and build a JAR:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker run -it --rm \
    -v $PWD/client/components:/source/client/components \
    -v $PWD/client/test:/source/client/test \
    -v $PWD/src:/source/src \
    -v $PWD/target/scala-2.10:/source/target/scala-2.10 \
    --env TEST_TYPE=all \
    localhost:5000/training-books-ms-tests</code></pre></div></div>
<div class="paragraph"><p>We are (or strive to become) experienced programmers. Important part of developing a service or an application are tests. Besides, it is hardly possible to create a continuous delivery pipeline without tests. Let me give you a short background of this service. It is a micro service containing both front-end and back-end. The service is part of a bigger system dedicated to selling stuff online. This particular micro service is in charge of retrieving and administrating books. Its goal is to provide web components that can be embedded into a Web site as well as back-end API that those components or a third party can invoke. Back-end is written with Scala with Spray and the front-end web components with Polymer JS library. On top of all that, it stores data in a MongoDB. Front-end tests should run in Chrome and Firefox, while back-end tests need a fresh instance of MongoDB. The complete list of everything needed for this service to be properly tested is as follows. MongoDB, NodeJS, Git, SBT, FireFox, Chrome, Scala, Gulp, Bower&#8230;&#8203; Since my memory does not serve me that well any more, there might be few more things I forgot. How much time do you think we would need to setup all those frameworks and tools on a new environment (probably Jenkins slave)? If that environment would be used for other things as well, how much headache we would have due to conflicts with the tools required by other services? Docker solves this, and many other things, by packaging everything in a container isolated from the rest of the environment. The server we are using, has only Docker and JDK installed (later is required for Jenkins slaves). You will find none of the tools we mentioned. Everything is inside the container we pulled. Let&#8217;s recap the goal of the command we are about to run. It should:</p></div>
<div class="olist arabic"><ol class="arabic"><li><p>Run Mongo DB</p></li><li><p>Run Back-end tests</p></li><li><p>Shut down Mongo DB</p></li><li><p>Run front-end tests using FireFox (optionally it can use Chrome as well since it is already inside the container)</p></li><li><p>Compile the code</p></li></ol></div>
<div class="paragraph"><p>Please execute the command you see on the screen and we&#8217;ll discuss the arguments while it is running. <em>MAKE SURE TO COMMENT WHAT IS HAPPENING ON THE SCREEN</em></p></div>
<div class="paragraph"><p>-it: Makes the container interactive with a terminal.
--rm: Removes the container when the process running inside it is finished. In other words, once the command is finished, there will be no trace of the container we run and resources will be freed for other tasks.
-v: Mounts a volume between the host and the container. In this case, we&#8217;re using it to mount the source code directories inside the container and, at the same time, to have the resulting JAR available.
--env: Sets environment variables. In this case, we&#8217;re using it to tell the code what type of tests to run.</p></div>
<div class="paragraph"><p>List directory contents:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">ll target/scala-2.10/</code></pre></div></div>
<div class="paragraph"><p>Let&#8217;s take a look whether the JAR was indeed created. If everything went as expected, we should have it inside the target/scala-2.10 directory. <em>RUN THE COMMAND</em>. Indeed, the books-ms-assembly-1.0.jar is there. All our tests run successfully and the JAR has been created.</p></div>
<div class="paragraph"><p>List Docker processes:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker ps -a | grep books</code></pre></div></div>
<div class="paragraph"><p>Let&#8217;s list all Docker processes and see what happened with the training-books-ms-tests container. and demonstrate that the container is completely gone as if it never existed. <em>RUN THE COMMAND</em>. As you can see, there is no trace of the container we run.</p></div>
<div class="paragraph"><p>The container is gone, code has been tested, and the JAR was built</p></div>
<div class="paragraph"><p>You will probably agree with me that the docker run command was too long. It would be close to impossible to remember all those arguments. How can we fix that? We could, for example, create a Shell script run_tests.sh. While that is a viable solution, there is a better one. We can use Docker Compose. However, I&#8217;m jumping in front of myself. Let us first explore a few more Docker commands.</p></div></aside></section><section id="docker_engine_building_containers_and_pushing_them_to_the_private_registry"><h2>Docker Engine: Building Containers and Pushing Them to the Private Registry</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">cat Dockerfile

docker build -t localhost:5000/training-books-ms .

docker push localhost:5000/training-books-ms</code></pre></div></div>
<aside class="notes"><div class="paragraph"><p>Building Containers and Pushing Them to the Private Registry</p></div>
<div class="paragraph"><p>Inspect Dockerfile:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">cat Dockerfile</code></pre></div></div>
<div class="paragraph"><p>Let&#8217;s take a look at Dockerfile. It contains the full definition Docker requires to build the container. Let&#8217;s take a look at it. [EXECUTE THE COMMAND]</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>FROM debian:jessie</code></pre></div></div>
<div class="paragraph"><p>FROM: Tells Docker what to use as a base image (in this case Debian). Explain that it is not a full Debian distribution but the absolute minimum for the rest of the instructions to run (for example apt-get). Docker uses host kernel so there is no need for the full Linux OS inside a container.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>MAINTAINER Viktor Farcic "viktor@farcic.com"</code></pre></div></div>
<div class="paragraph"><p>MAINTAINER is for informational purposes only.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>RUN apt-get update &amp;&amp; \
    apt-get install -y --force-yes --no-install-recommends openjdk-7-jdk &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*</code></pre></div></div>
<div class="paragraph"><p>RUN executes a command in the same way we&#8217;d run it through shell. In this case, it updates the system, installs JDK7, and, finally, performs some cleanup. Please note that multiple commands were specified. While we could have split it into several RUNs, using this syntax keeps the container small. Since each instruction is converted into a separate immutable image, if we did use multiple RUNs, deleting files with rm would have no effect of the container size. The apt-get update would generate an image and rm would create another.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>ENV DB_DBNAME books
ENV DB_COLLECTION books
ENV DB_HOST localhost</code></pre></div></div>
<div class="paragraph"><p>ENV sets environment variables. In this case, we are creating several that will be used by the service. Those are default values that can be changed at runtime. Please note that environment variables are a preferred way to pass some custom values to a container. Alternative would be to expose a directory inside the container and inject parameter files. This alternative would jeopardise immutability and complicate deployments.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>COPY run.sh /run.sh</code></pre></div></div>
<div class="paragraph"><p>The COPY command copies a file or a directory from the host to the container image.  In this case, we are copying the script from the source code to the container, and changing its permissions.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>RUN chmod +x /run.sh</code></pre></div></div>
<div class="paragraph"><p>The RUN command runs any shell script or commands.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>COPY target/scala-2.10/books-ms-assembly-1.0.jar /bs.jar
COPY client/components /client/components</code></pre></div></div>
<div class="paragraph"><p>We need a few more files. Namely, the JAR that we build as part of the testing process and some front-end static files. This might be a good moment to speak about the order of instructions inside Dockerfile. As a rule of thumb, you want to put those that are less likely to change at the top. In this case, installation of packages is highly unlikely to change, the environment variables and the run.sh script somewhat unlikely, while the JAR file and the static files are very likely to change on each build. The reason behind this order lies in the way Docker builds containers. It will start from the first instruction and go down, one by one, building images. If it finds the image corresponding to an instruction in its cache, that image will be reused. However, the first time it decides that a new image should be built, it will continue creating new ones from that instruction all the way until the end of Dockerfile.</p></div>
<div class="paragraph"><p>We&#8217;re almost done. Two more instructions are left.</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>CMD ["/run.sh"]</code></pre></div></div>
<div class="paragraph"><p>CMD represents a command that will be run when container is started. This can be overwritten at run time</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code>EXPOSE 8080</code></pre></div></div>
<div class="paragraph"><p>EXPOSE defines ports that will be exposed. Please note that each container is like a host in itself. If, for example, you run a service inside a container and that service listens on port 8080, that port is opened only internally inside the container. Unless specified through EXPOSE, the host container is running on will not know about that port. The EXPOSE instruction tells Docker what are the ports that should be exposed to the host. As you&#8217;ll see later, we have the option to defined to which port it will map to or let Docker decide. This combination of mapping between internal and external ports allows us not to worry that there will be conflicts (after all, there can be no two processes listening on the same port). You&#8217;ll see ports exposure in action very soon.</p></div>
<div class="paragraph"><p>Please note that the Dockerfile instructions we explored are, by no means, all available. We won&#8217;t have time to go through all of them during this training. I encourage you to consult the documentation yourself.</p></div>
<div class="paragraph"><p>Build the container for private registry:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker build -t localhost:5000/training-books-ms .</code></pre></div></div>
<div class="paragraph"><p>Now that we understand how to define our containers through Dockerfile, let&#8217;s see how we can build a new version of our container. The build command creates a new container. In this case, we are using the -t argument to specify the tag of the container in registry-friendly way. Please note that there is a dot at the end of the command. The last argument specifies the directory where Dockerfile is located. Since it is in the same folder as the one we are right now, we use dot. <em>RUN THE COMMAND</em>. You&#8217;ll notice that the first eight steps have the Using cache comment. Docker compared this Dockerfile instructions with the cache and decided that there is no need to build new images, thus speeding up the process and, at the same time, saving some disk space. Starting from the step nine, new images had to be built since we created a new JAR file that does not match the one Docker had in its cache. This was a practical demonstration of the need to give a careful thought to the order of Dockerfile instructions.</p></div>
<div class="paragraph"><p>Push the container to private registry:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker push localhost:5000/training-books-ms</code></pre></div></div>
<div class="paragraph"><p>We built a new container but, at the moment, it is available only on the same host. If, for example, we&#8217;d like to run it on some other server, we&#8217;d need to first store it in the registry. The push command does that for us. It pushes the specified container to a registry. If the tag is prefixed with IP/port combination, Docker will assume that is the address of the registry. Otherwise, it would push the container to Docker Hub. <em>RUN THE COMMAND</em>. Please observe that only some of the images were actually pushed. Those that already exist were skipped.</p></div>
<div class="paragraph"><p>The new release is ready to be pulled from any server</p></div></aside></section><section id="mid_break_2"><h2>Mid-Break</h2><div class="paragraph"><p>(10) minutes for learner re-integration.</p></div>
<div class="imageblock" style=""><div class="content"><img src="./images/break.png" alt="break" width="300"></div></div></section><section id="docker_engine_running_containers"><h2>Docker Engine: Running Containers</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker run -d --name books-ms-db mongo

docker run -d --name books-ms \
    -p 1234:8080 \
    --link books-ms-db:db \
    localhost:5000/training-books-ms

docker exec -it books-ms env | grep DB

docker ps

curl -I localhost:1234/api/v1/books</code></pre></div></div>
<aside class="notes"><div class="paragraph"><p>Running containers</p></div>
<div class="paragraph"><p>Run the MongoDB container:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker run -d --name books-ms-db mongo</code></pre></div></div>
<div class="paragraph"><p>The service we are using relies on MongoDB as data storage. While we could have placed it inside the container we built, that would potentially create problems. For example, scaling a single, do-it-all container is more difficult. We might realise that we need two instances of the service itself and only one of the DB. There are other reasons but we won&#8217;t go into them. The important thing to note is that we need both the service and the MongoDB running as two separate containers. Since, in this case, we do not need anything special from the MongoDB, we&#8217;ll run the official, publicly available, container from Docker Hub. Let&#8217;s run the DB first. <em>RUN THE COMMAND</em>. This is the command in its, almost, simplest form. We specified that the instance should run in detached mode (in the background), that it should be named books-ms-db, and, finally, the name of the container (mongo). Please note that we did not specify any port for the mongo container. You might be asking how are we going to invoke it from our service? The answer lies in links, as you&#8217;ll see in the next command.</p></div>
<div class="paragraph"><p>Run the service container linked to MongoDB:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker run -d --name books-ms \
    -p 1234:8080 \
    --link books-ms-db:db \
    localhost:5000/training-books-ms</code></pre></div></div>
<div class="paragraph"><p>We&#8217;ll use the same -d and --name arguments to specify that this container should also run in detached mode and be named books-ms. The -p argument specifies that the internal port 8080 should be exposed as 1234 on the host. Once we run it, our service will be available on the port 1234 which will be internally mapped to the port 8080. The next in line is the --link that will create a connection between the books-ms-db and the book-ms containers. Docker will create environment variables that we&#8217;ll be able to use to find out the internal IP and the port of the MongoDB. Finally, we set that the container that should be run is localhost:5000/training-books-ms. <em>RUN THE COMMAND</em></p></div>
<div class="paragraph"><p>List environment variables inside the running container:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker exec -it books-ms env | grep DB</code></pre></div></div>
<div class="paragraph"><p>Let&#8217;s see the environment variables Docker created through the --link instruction and, at the same time, explore one more command. The exec command allows us to execute a command inside a running container. In this case, we&#8217;re executing env that will list all environment variables and piping the result to grep DB so that only those we are interested with are output. <em>RUN THE COMMAND</em>. As you can see, Docker created quite a few variables so that, depending on our needs, we can choose the one (or more) that suit us the best. The important thing to note is that Docker created an internal network with the IP 172.17.0.3. Through it, we can access the DB from this container even though the same IP does not exist in the host. In other words, the database is accessible only through this container.</p></div>
<div class="paragraph"><p>List all running Docker process:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker ps</code></pre></div></div>
<div class="paragraph"><p>We can leverage the ps command to list all Docker processes and confirm that our containers are indeed running. <em>RUN THE COMMAND</em>. As you can see, we are currently running three containers. Each of the is described with the container&#8217;s unique identification (ID), the name of the image, command used to start it, how long ago it was created, the status, exposed ports, and, finally, names. Besides the two containers we just run (books-ms and books-ms-db), you can also see the registry where we pushed the container after it was built.</p></div>
<div class="paragraph"><p>Confirm that the service works:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">curl -I localhost:1234/api/v1/books</code></pre></div></div>
<div class="paragraph"><p>Finally, let&#8217;s double check that the containers we run are indeed working. Since the books-ms service depends on MongoDB container, we can send a request that queries the DB and, it the HTTP response status is 200, everything is working as expected. <em>RUN THE COMMAND</em>. Success! Our service inside the container is indeed working.</p></div>
<div class="paragraph"><p>The service packed inside a Docker container has been deployed</p></div>
<div class="paragraph"><p>Shall we take a look at a few more Docker commands?</p></div></aside></section><section id="docker_engine_logging_stopping_and_removing_containers"><h2>Docker Engine: Logging, Stopping and Removing Containers</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker logs books-ms

docker logs books-ms-db

docker stop books-ms-db books-ms

docker ps -a

docker rm books-ms-db books-ms

docker ps -a</code></pre></div></div>
<aside class="notes"><div class="paragraph"><p>Logging, Stopping and Removing Containers</p></div>
<div class="paragraph"><p>Outputting service logs</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker logs books-ms</code></pre></div></div>
<div class="paragraph"><p>Outputting DB logs:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker logs books-ms-db</code></pre></div></div>
<div class="paragraph"><p>Displaying logs of a running container is easy. All we have to do is run the logs command followed with the name of the ID of the container. [RUN COMMANDS]. Please note that we could use the -f argument. In that case, Docker would tail logs displaying them in near-realtime. It&#8217;s worth mentioning that the best practice is to write the code that sends logs to standard output. If needed, we can always instruct Docker to redirect them to a file or, for example, syslog.</p></div>
<div class="paragraph"><p>Stopping containers:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker stop books-ms-db books-ms</code></pre></div></div>
<div class="paragraph"><p>We can just as easily stop a running container. <em>RUN THE COMMAND</em></p></div>
<div class="paragraph"><p>Listing all Docker processes:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker ps -a</code></pre></div></div>
<div class="paragraph"><p>To display all (not only running) processes, please use the -a argument. <em>RUN THE COMMAND</em>. Since we stopped our containers, their status is EXITED.</p></div>
<div class="paragraph"><p>Removing containers:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker rm books-ms-db books-ms</code></pre></div></div>
<div class="paragraph"><p>We can also remove containers with the --rm argument. <em>RUN THE COMMAND</em>.</p></div>
<div class="paragraph"><p>Listing all Docker processes:</p></div>
<div class="listingblock"><div class="content"><pre class="highlight"><code class="bash language-bash">docker ps -a</code></pre></div></div>
<div class="paragraph"><p><em>RUN THE COMMAND</em>. This time, our containers are absent from the list of all processes.</p></div>
<div class="paragraph"><p>The service has been removed from the server</p></div>
<div class="paragraph"><p>Let us explore few other tools available in the Docker ecosystem. We won&#8217;t have much time to spend with them. Instead, I&#8217;ll give you only a brief overview that you can leverage later on when you explore Docker in more depth.</p></div></aside></section><section id="docker_compose"><h2>Docker Compose</h2><div class="imageblock right top" style=""><div class="content"><img src="./images/docker-compose.png" alt="docker compose" width="200"></div></div>
<div class="ulist"><ul><li><p><strong>Docker Compose</strong> allows defining multi-container application with all of its dependencies in a single file, then spin your application up in a single command.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Docker Compose allows defining multi-container application with all of its dependencies in a single file, then spin your application up in a single command. The main advantage is that it allows us to specify in an easy to read and write format everything our containers need for running. Instead of remembering all the arguments and dependencies we used previously, with Docker Compose we would run short single commands like docker-compose up, docker-compose stop, and so on. Since everything is defined in a docker-compose.yml file, compose would pass detailed instructions to the Engine.</p></div></aside></section><section id="docker_swarm"><h2>Docker Swarm</h2><div class="imageblock right top" style=""><div class="content"><img src="./images/docker-swarm.png" alt="docker swarm" width="200"></div></div>
<div class="ulist"><ul><li><p><strong>Docker Swarm</strong> provides native clustering capabilities to turn a group of Docker engines into a single, virtual Docker Engine. With these pooled resources, you can scale out your application as if it were running on a single, huge computer.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Docker Swarm provides native clustering capabilities to turn a group of Docker engines into a single, virtual Docker Engine. With these pooled resources, you can scale out your application as if it were running on a single, huge computer. Running a few applications packed inside several containers is easy. Doing the same with tens, hundreds, or even thousands, is quite challenging. Where should we deploy a service? Which server has available resources. How to scale? What happens in case a node is down? Those, and many other scenarios, can be solved with Docker Swarm. It, or a similar tool like Mesos or Kubernetes, is indispensable in any but smallest datacenters.</p></div></aside></section><section id="docker_machine"><h2>Docker Machine</h2><div class="imageblock right top" style=""><div class="content"><img src="./images/docker-machine.png" alt="docker machine" width="175"></div></div>
<div class="ulist"><ul><li><p><strong>Docker Machine</strong> automatically sets up Docker on your computer, on cloud providers, and inside your data center. Docker Machine provisions the hosts, installs Docker Engine on them, and then configures the Docker client to talk to the Docker Engines.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Docker Machine automatically sets up Docker on your computer, on cloud providers, and inside your data center. Docker Machine provisions the hosts, installs Docker Engine on them, and then configures the Docker client to talk to the Docker Engines.</p></div></aside></section><section id="kitematic"><h2>Kitematic</h2><div class="imageblock right top" style=""><div class="content"><img src="./images/kitematic.png" alt="kitematic" width="250"></div></div>
<div class="ulist"><ul><li><p><strong>Kitematic</strong> is a completely automated process that installs and configures the Docker environment on your machine. Build and run containers through a simple, yet powerful graphical user interface (GUI).</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Kitematic is a completely automated process that installs and configures the Docker environment on your machine. Build and run containers through a simple, yet powerful graphical user interface (GUI).</p></div></aside></section><section id="docker_introduction_review"><h2>Docker Introduction: Review</h2></section><section id="docker_introduction_review_2"><h2>Docker Introduction: Review</h2><div class="ulist"><ul><li><p>Docker benefits &amp; advantages</p></li><li><p>Docker use cases</p></li><li><p>Docker Hub and Registry, Engine, Compose, Swarm, Machine, Kitematic</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Docker benefits &amp; advantages:</strong></p></div>
<div class="paragraph"><p>Some of the Docker benefits we discussed is self-sufficiency, isolation, immutability, reliability, and scalability. We showed how it differs from virtual machines and why containers use much less resources. We also discussed few of the ways we can organise our services and applications. They can be fully self-sufficient with everything inside a single container (API, application layers, database, system and runtime libraries, and so on. We also discussed that in many cases it is a better solution to split an application into multiple containers with, for example, service running in one and a database in another.</p></div>
<div class="paragraph"><p><strong>Docker use cases:</strong></p></div>
<div class="paragraph"><p>We discussed some of the most common use cases like local development and testing, continuous integration, delivery, or deployment, reliable and repeatable deployments, and so on.</p></div>
<div class="paragraph"><p>From there on we jumped into some of the tools in the Docker ecosystem.</p></div>
<div class="paragraph"><p>Docker Hub and Registry
We pulled an image from Docker Hub and, later on, pushed it to the private registry running on our server.</p></div>
<div class="paragraph"><p><strong>Docker Engine:</strong></p></div>
<div class="paragraph"><p>We explored some of the common commands available in the Docker Engine. We explored Dockerfile that we used to define all the steps Engine needs to build the service container which, later on, we pushed to the private registry. We run the MongoDB container that served as a database for your service which we also run as a container. We learned how to enter a running container as well as how to list all Docker processes. We explored commands that help us display logs, and stop and remove a container.</p></div>
<div class="paragraph"><p><strong>Docker Compose:</strong></p></div>
<div class="paragraph"><p>We briefly went through Docker Compose that can be used to specify all the parameters required to operate containers.</p></div>
<div class="paragraph"><p><strong>Docker Swarm:</strong></p></div>
<div class="paragraph"><p>We discussed the usage of Docker Swarm as the containers orchestrator inside a datacenter.</p></div>
<div class="paragraph"><p><strong>Docker Machine:</strong></p></div>
<div class="paragraph"><p>We saw that Docker Machine can be utilised to quickly create virtual machines ready for usage within the Docker Ecosystem.</p></div>
<div class="paragraph"><p><strong>Kitematic:</strong></p></div>
<div class="paragraph"><p>Finally, we mentioned that on Windows and OS X operating systems, we can use Kitematic to configure our system to run Docker tools. It also provides a useful UI.</p></div></aside></section><section id="docker_introduction_exercise"><h2>Docker Introduction: Exercise</h2><div class="paragraph"><p><a href="labs.html#_docker_introduction_exercise">Docker Introduction: Exercise</a></p></div></section></section>
<section><section id="the_project_part_1_day_3"><h2>The Project - Part 1 (Day 3)</h2></section><section id="review_of_day_2_concepts_and_exercise"><h2>Review Of Day 2 Concepts And Exercise</h2><div class="ulist"><ul><li><p>Docker benefits &amp; advantages</p></li><li><p>Docker use cases</p></li><li><p>Docker Hub and Registry, Engine, Compose, Swarm, Machine, Kitematic</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Docker benefits &amp; advantages:</strong></p></div>
<div class="paragraph"><p>Some of the Docker benefits we discussed is self-sufficiency, isolation, immutability, reliability, and scalability. We showed how it differs from virtual machines and why containers use much less resources. We also discussed few of the ways we can organise our services and applications. They can be fully self-sufficient with everything inside a single container (API, application layers, database, system and runtime libraries, and so on. We also discussed that in many cases it is a better solution to split an application into multiple containers with, for example, service running in one and a database in another.</p></div>
<div class="paragraph"><p><strong>Docker use cases:</strong></p></div>
<div class="paragraph"><p>We discussed some of the most common use cases like local development and testing, continuous integration, delivery, or deployment, reliable and repeatable deployments, and so on.</p></div>
<div class="paragraph"><p>From there on we jumped into some of the tools in the Docker ecosystem.</p></div>
<div class="paragraph"><p><strong>Docker Hub and Registry:</strong></p></div>
<div class="paragraph"><p>We pulled an image from Docker Hub and, later on, pushed it to the private registry running on our server.</p></div>
<div class="paragraph"><p><strong>Docker Engine:</strong></p></div>
<div class="paragraph"><p>We explored some of the common commands available in the Docker Engine. We explored Dockerfile that we used to define all the steps Engine needs to build the service container which, later on, we pushed to the private registry. We run the MongoDB container that served as a database for your service which we also run as a container. We learned how to enter a running container as well as how to list all Docker processes. We explored commands that help us display logs, and stop and remove a container.</p></div>
<div class="paragraph"><p><strong>Docker Compose:</strong></p></div>
<div class="paragraph"><p>We briefly went through Docker Compose that can be used to specify all the parameters required to operate containers.</p></div>
<div class="paragraph"><p><strong>Docker Swarm:</strong></p></div>
<div class="paragraph"><p>We discussed the usage of Docker Swarm as the containers orchestrator inside a datacenter.</p></div>
<div class="paragraph"><p><strong>Docker Machine:</strong></p></div>
<div class="paragraph"><p>We saw that Docker Machine can be utilised to quickly create virtual machines ready for usage within the Docker Ecosystem.</p></div>
<div class="paragraph"><p><strong>Kitematic:</strong></p></div>
<div class="paragraph"><p>Finally, we mentioned that on Windows and OS X operating systems, we can use Kitematic to configure our system to run Docker tools. It also provides a useful UI.</p></div>
<div class="paragraph"><p><strong>Exercise:</strong></p></div>
<div class="paragraph"><p><em>TEACHER DISPLAYS AND COMMENTS ON A FEW OF THE RESULTS PARTICIPANTS SENT</em></p></div></aside></section><section id="in_this_unit_you_will_learn_3"><h2>In This Unit: You Will Learn</h2><div class="ulist"><ul><li><p>How to combine CloudBees Pipeline plugin with Docker</p></li><li><p>How to implement most commonly used steps required for CI/CD flow</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Today, we&#8217;ll start working on a project that combined CloudBees Jenkins Pipeline Plugin with Docker. The idea behind today&#8217;s and tomorrow&#8217;s sessions is to learn basics about Jenkins Pipeline and Docker Pipeline plugins combined.</p></div>
<div class="paragraph"><p>Learn how to combine CloudBees Pipeline plugin with Docker:</p></div>
<div class="paragraph"><p>We&#8217;ll put the Docker commands we learned in use to perform operations like building, pushing, pulling, running, and so on. We&#8217;ll do all that, and more, through the Pipeline plugin.</p></div></aside></section><section id="in_this_unit_you_will_be_able_to_3"><h2>In This Unit: You Will Be Able To</h2><div class="ulist"><ul><li><p>Create deployment lifecycle with Jenkins Pipeline and Docker</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Create deployment lifecycle with Jenkins Pipeline and Docker</p></div>
<div class="paragraph"><p>We&#8217;ll combine Jenkins Pipeline and Docker to produce an effective and easy to maintain solution for the full continuous deployment lifecycle.</p></div>
<div class="paragraph"><p>Let&#8217;s to through the steps we&#8217;ll perform as part of the deployment pipeline.</p></div></aside></section><section id="the_project"><h2>The Project</h2><div class="ulist"><ul><li><p>Run pre-deployment tests inside a Docker container</p></li><li><p>Build artefacts</p></li><li><p>Build and push the service container</p></li><li><p>Request manual permission to deploy the service container to production</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Run pre-deployment tests inside a Docker container:</strong></p></div>
<div class="paragraph"><p>Run pre-deployment tests inside a Docker container. We&#8217;ll see how we can utilise the combination of the Jenkins Pipeline with Docker to run all kinds of tests. We&#8217;ll run front-end unit tests against Firefox and back-end functional tests that depends on MongoDB. We&#8217;ll call them pre-deployment tests.</p></div>
<div class="paragraph"><p><strong>Build artefacts:</strong></p></div>
<div class="paragraph"><p>Once our pre-deployment tests are successful, we&#8217;ll run the process that will build a JAR artefact and prepare front-end static files for packaging.</p></div>
<div class="paragraph"><p><strong>Build and push the service container:</strong></p></div>
<div class="paragraph"><p>With the artefacts ready, we&#8217;ll proceed, and build the container image that will host our service. Once built, we&#8217;ll push it to the private registry so that it is available for anyone with the access to our server.</p></div>
<div class="paragraph"><p><strong>Request manual permission to deploy the service container to production:</strong></p></div>
<div class="paragraph"><p>Next, we&#8217;ll add a process that will require a manual confirmation before the container is deployed to production.</p></div></aside></section><section id="the_project_cont"><h2>The Project (cont.)</h2><div class="ulist"><ul><li><p>Pull the service container and all dependent containers to production</p></li><li><p>Run the service container and all dependent containers in production</p></li><li><p>Run post-deployment tests (integration tests) inside a Docker container</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Pull the service container and all dependent containers to production:</strong></p></div>
<div class="paragraph"><p>Once we receive the confirmation, we&#8217;ll switch to the production server and pull the latest release from the registry</p></div>
<div class="paragraph"><p><strong>Run the service container and all dependent containers in production:</strong></p></div>
<div class="paragraph"><p>With the latest released pulled, we&#8217;ll proceed and run the service container as well as those it depends on.</p></div>
<div class="paragraph"><p><strong>Run post-deployment tests (integration tests) inside a Docker container:</strong></p></div>
<div class="paragraph"><p>Finally, we&#8217;ll run another round of tests to confirm that the process was indeed executed correctly. We&#8217;ll call them post-deployment or integration tests.</p></div>
<div class="paragraph"><p>How does that sound for a project? Shall we give it a try?</p></div></aside></section><section id="reality_check_3"><h2>Reality Check</h2><div class="ulist"><ul><li><p>Questions on the preparation and the project?</p></li><li><p>Does this workflow differ from your practices?</p></li><li><p>Using containers now?</p></li></ul></div></section><section id="create_a_pipeline_job_called_my_pipeline"><h2>Create a Pipeline Job Called my-pipeline</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-create-my-pipeline.png" alt="cje create my pipeline" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>A new Pipeline job can be created by clicking the "New Item" link from the left-hand menu, typing a name of the job, selecting "Pipeline" as type and clicking the "OK" button. For this exercise, please use "my-pipeline" as the job name.</p></div></aside></section><section id="key_pipeline_dsl_node_task"><h2>Key Pipeline DSL – node: Task</h2><div class="paragraph"><p>Specify the node <em>cd</em>, run the pipeline, and confirm that it is running inside the <em>cd</em> node by looking at logs.</p></div>
<aside class="notes"><div class="paragraph"><p>A node is a step that schedules a task to run by adding it to the Jenkins build queue. As soon as an executor slot is available on a node (the Jenkins master, or a slave), the task is run on that node. A node also allocates a workspace (file directory) on that node for the duration of the task (more on this later). The argument inside brackets can be used to define the name of the node or label. In this case, the code inside this node will run only on those named or with the label "cd".</p></div>
<div class="paragraph"><p>To see all the available arguments, please click the "Snippet Generator" checkbox and select the "node" step. To generate the Groovy code, please write "cd" in the "Label" field and click the "Generate Groovy" button. Generated code can be copy&amp;pasted into the "Script" text box. Same process should be followed for the rest of exercises.</p></div></aside></section><section id="key_pipeline_dsl_node_solution"><h2>Key Pipeline DSL – node: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">node("cd") {
}</code></pre></div></div></section><section id="run_the_job"><h2>Run the Job</h2><div class="paragraph"><p>Open http://<strong>&lt;IP&gt;</strong>:8080/job/my-pipeline/build?delay=0sec</p></div>
<div class="paragraph"><p>Open http://<strong>&lt;IP&gt;</strong>:8080/job/my-pipeline/lastBuild/console</p></div>
<div class="imageblock" style=""><div class="content"><img src="./images/cje-workflow-console.png" alt="cje workflow console" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>Pipeline jobs are run as any other type of Jenkins jobs. Click the "Build Now" link in the left-hand menu when inside the job screen or open the first URL presented on the screen. Let&#8217;s take a look at the console output of the build (the second URL). Not much happened since we haven&#8217;t specified any instruction to be run inside the node. The important thing to note is that we can see from the output that the job was run inside the "node-cd" node. The node is configured with the"cd" label.</p></div></aside></section><section id="key_pipeline_dsl_git_task"><h2>Key Pipeline DSL – git: Task</h2><div class="paragraph"><p>Clone the code from the repository <em><a href="https://github.com/cloudbees/training-books-ms.git" class="bare">https://github.com/cloudbees/training-books-ms.git</a></em></p></div>
<aside class="notes"><div class="paragraph"><p>Git step:</p></div>
<div class="paragraph"><p>It performs a clone from the specified repository. We&#8217;ll use it to get the latest code from the Git repository cloudbees/training-books-ms. Please note that the git step is intelligent enough to know whether the code should be cloned or pulled from the repository.</p></div></aside></section><section id="key_pipeline_dsl_git_solution"><h2>Key Pipeline DSL – git: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">    git "https://github.com/cloudbees/training-books-ms.git"</code></pre></div></div></section><section id="key_pipeline_dsl_variables_pwd_and_sh_task"><h2>Key Pipeline DSL – variables, pwd and sh: Task</h2><div class="paragraph"><p>Assign the current job workspace directory to the <em>dir</em> variable, create directory <em>db</em> inside the workspace, and assign full permissions to all users.</p></div>
<aside class="notes"><div class="paragraph"><p>Variables have the same function as in any other programming or scripting language. In addition, all job parameters are available as Pipeline variables. Inside a string, variable values can be obtained using the same syntax as in bash scripts by prefixing them with the dollar sign. Curly braces are optional.
The "pwd" function returns the current directory path as a string.
The "sh" runs a Bourne shell script on a Unix node. Multiple lines are accepted. An interpreter selector may be used, for example: "#!/usr/bin/perl". We can use it to create the directory by running "mkdir" and assign permissions with "chmod".</p></div></aside></section><section id="key_pipeline_dsl_variables_pwd_and_sh_solution"><h2>Key Pipeline DSL – variables, pwd and sh: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">    def dir = pwd()
    sh "mkdir -p ${dir}/db"
    sh "chmod 0777 ${dir}/db"</code></pre></div></div></section><section id="key_pipeline_dsl_stage_task"><h2>Key Pipeline DSL – stage: Task</h2><div class="paragraph"><p>Create the <em>pre-deployment tests</em> stage.</p></div>
<aside class="notes"><div class="paragraph"><p>By default, flow builds can run concurrently. The stage command lets you mark certain sections of a build as being constrained by limited concurrency. Newer builds are always given priority when entering such a throttled stage; older builds will simply exit early if they are preempted. Stage is also used inside the "Stage View" as a way to separate the flow into groups.</p></div></aside></section><section id="key_pipeline_dsl_stage_solution"><h2>Key Pipeline DSL – stage: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">    stage "pre-deployment tests"</code></pre></div></div></section><section id="mid_break_3"><h2>Mid-Break</h2><div class="paragraph"><p>(10) minutes for learner re-integration.</p></div>
<div class="imageblock" style=""><div class="content"><img src="./images/break.png" alt="break" width="300"></div></div></section><section id="key_pipeline_dsl_docker_task"><h2>Key Pipeline DSL – docker: Task</h2><div class="paragraph"><p>Pull the Docker image <em>localhost:5000/books-ms-tests</em> and run the <em>run_tests.sh</em> script inside the container.  Host volume <em>db</em> should be mounted as <em>/data/db</em> inside the container.</p></div>
<aside class="notes"><div class="paragraph"><p>Docker image we want to work with can be specified with <code>docker.image('FULL_IMAGE_NAME')</code>. It will return a reference that can be stored as a variable. Images can be pulled from Registry using ".pull()". Use ".inside" to run instructions inside a container. The "inside" function accepts arguments that will be passed to Docker. For example, we can specify volume like <code>-v HOST_VOLUME_PATH:DOCKER_VOLUME_PATH</code>.</p></div></aside></section><section id="key_pipeline_dsl_docker_solution"><h2>Key Pipeline DSL – docker: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">    def tests = docker.image("localhost:5000/training-books-ms-tests")
    tests.pull()
    tests.inside("-v ${dir}/db:/data/db") {
        sh "./run_tests.sh"
    }</code></pre></div></div></section><section id="key_pipeline_dsl_docker_task_2"><h2>Key Pipeline DSL – docker: Task</h2><div class="paragraph"><p>Build the Docker image <em>localhost:5000/books-ms</em> and push the container to the private registry. Use the stage <em>build</em> for these steps.</p></div>
<aside class="notes"><div class="paragraph"><p>Building an image can be done with ".build" followed by the image name. The result can be assigned to a variable. Similarly, image can be pushed through ".push()".</p></div></aside></section><section id="key_pipeline_dsl_docker_solution_2"><h2>Key Pipeline DSL – docker: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">    stage "build"
    def service = docker.build "localhost:5000/training-books-ms"
    service.push()</code></pre></div></div></section><section id="key_pipeline_dsl_stash_and_unstash_task"><h2>Key Pipeline DSL – stash and unstash: Task</h2><div class="paragraph"><p>Pull containers and run (through Docker Compose target <em>app</em>) the <em>localhost:5000/books-ms</em> container in the <em>production</em> node. Use <em>stash</em> to archive <em>docker-compose-dev.yml</em> file while in the <em>cd</em> node and <em>unstash</em> to retrieve it when inside the <em>production</em> node. Before running the service, make sure that both the service and mongo containers are pulled.</p></div>
<aside class="notes"><div class="paragraph"><p>If multiple nodes are used inside the same Pipeline, in some cases there is the need to move files from one to another. The "stash" instructions stores some files to be used later in the build. Its counterpart is "unstash" that restores stashed files.</p></div></aside></section><section id="key_pipeline_dsl_stash_and_unstash_solution"><h2>Key Pipeline DSL – stash and unstash: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">node("cd") {
...
    stash includes: "docker-compose*.yml", name: "docker-compose"
}
node("production") {
    stage "deploy"
    unstash "docker-compose"
    docker.image("localhost:5000/training-books-ms").pull()
    docker.image("mongo").pull()
    sh "docker-compose -p books-ms up -d app"
}</code></pre></div></div></section><section id="key_pipeline_dsl_env_and_withenv_task"><h2>Key Pipeline DSL – env and withEnv: Task</h2><div class="paragraph"><p>Run post-deployment tests in the node <em>cd</em>. The <em>run_tests.sh</em> script expects two environment variables: <em>TEST_TYPE=integ</em> and <em>DOMAIN=<strong>&lt;IP&gt;</strong>:8081</em>. Tests should be run inside the <em>training-books-ms</em> container.</p></div>
<aside class="notes"><div class="paragraph"><p>Environment variables can be assigned or retrieved using the "env" instruction. The "withEnv" instruction sets one or more environment variables within a block. It is a very useful feature that allows us to limit the scope of environment variables. Please note that [IP] should be replaced with the IP of your AWS instance.</p></div></aside></section><section id="key_pipeline_dsl_env_and_withenv_solution"><h2>Key Pipeline DSL – env and withEnv: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">node("cd") {
    stage "post-deployment tests"
    def tests = docker.image("localhost:5000/training-books-ms-tests")
    tests.inside() {
        withEnv(["TEST_TYPE=integ", "DOMAIN=http://[IP]:8081"]) {
            sh "./run_tests.sh"
        }
    }
}</code></pre></div></div></section><section id="the_project_part_1_review"><h2>The Project - Part 1: Review</h2></section><section id="the_project_part_1_review_2"><h2>The Project - Part 1: Review</h2><div class="ulist"><ul><li><p>Defined the project steps</p></li><li><p>Created a new Pipeline job</p></li><li><p>Defined steps that clone the code</p></li><li><p>Defined steps that run pre-deployment tests</p></li><li><p>Defined steps that build and push the container</p></li><li><p>Defined steps that deploy the container</p></li><li><p>Defined steps that run post-deployment tests</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p><strong>Defined the project steps:</strong></p></div>
<div class="paragraph"><p>We defined the steps that will constitute out project. In a nutshell, the project will perform all the phases of the deployment flow, starting from testing and building, all the way until the service is deployed and integrated in production.</p></div>
<div class="paragraph"><p><strong>Created a new Pipeline job:</strong></p></div>
<div class="paragraph"><p>We created a new Jenkins Pipeline job that will be used to perform all the steps we defined.</p></div>
<div class="paragraph"><p><strong>Defined Pipeline steps:</strong></p></div>
<div class="paragraph"><p>We started by specifying the node our pipeline will run in, made sure that the code is retrieved from the repository, defined few variables, created a directory and set proper permissions through the sh step, and set the pre-deployment tests stage. As part of the stage, we pulled the latest container with the tests, entered it and run part of the tests suite. From there on, we built a new container with the latest release of the service and pushed it to the private registry. We had to use stash and unstash steps to move few files into the production server and defined the steps that form our deploy stage. Once inside the production server, we pulled the service container together with the MongoDB and used Docker Compose to run both of them. Finally, we run post-deployment tests that proved that the release was successfully deployed and integrated.</p></div></aside></section><section id="the_project_part_1_exercise"><h2>The Project - Part 1: Exercise</h2><div class="paragraph"><p><a href="labs.html#_the_project_part_1_exercise">The Project - Part 1: Exercise</a></p></div></section></section>
<section><section id="the_project_part_2_day_4"><h2>The Project - Part 2 (Day 4)</h2></section><section id="review_of_day_3"><h2>Review Of Day 3</h2><div class="ulist"><ul><li><p>Defined the project steps</p></li><li><p>Created a new Pipeline job</p></li><li><p>Defined steps that clone the code</p></li><li><p>Defined steps that run pre-deployment tests</p></li><li><p>Defined steps that build and push the container</p></li><li><p>Defined steps that deploy the container</p></li><li><p>Defined steps that run post-deployment tests</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>We did quite a lot yesterday. We created a new Pipeline job, setup a few nodes where our tasks will run. We used one node for all pre-deployment tasks and the other to deploy our service. We checked out the code from Git and declared a variable holding the path to the current directory. We used that variable to create a new directory and assign permissions through the sh step. We split the steps into a few stages; pre-deployment tests, build, deploy, and post-deployment tests. We pulled the Docker image containing all our tests and used the Pipeline step inside to run them. That run resulted in a JAR file containing the new release of the service. We used it to build a new Docker image containing the release and pushed it to the private registry. From there on, we deployed the new release to the production server and run post-deployment tests that ensured the release was indeed deployed correctly.</p></div>
<div class="paragraph"><p>You got the assignment to apply the knowledge into your projects. Let&#8217;s take a look at a few examples you sent me. <em>INSTRUCTOR SHOWS A COUPLE OF EXAMPLES</em>.</p></div></aside></section><section id="in_this_unit_you_will_learn_4"><h2>In This Unit: You Will Learn</h2><div class="ulist"><ul><li><p>How to use more advanced features of the CloudBees Pipeline plugin and Docker</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Yesterday, we finished a simple implementation of the deployment lifecycle project using the CloudBees Pipeline and Docker. Today, we&#8217;ll work on some of the advanced features we can leverage to make the deployment flow even more robust. As you&#8217;ll see very soon, more advanced does not necessarily mean more complicated. Indeed, we tried our best to make all the features of the Pipeline easy to learn and use.</p></div></aside></section><section id="in_this_unit_you_will_be_able_to_4"><h2>In This Unit: You Will Be Able To</h2><div class="ulist"><ul><li><p>Create advanced deployment lifecycle with Jenkins Pipeline and Docker</p></li></ul></div></section><section id="the_project_cont_2"><h2>The Project (cont.)</h2><div class="ulist"><ul><li><p>Request manual confirmation before deployment</p></li><li><p>Run steps in parallel</p></li><li><p>Control the steps execution</p></li><li><p>Add checkpoints on critical or after long running processes</p></li><li><p>Visualize the deployment pipeline</p></li><li><p>Convert the job into a template</p></li><li><p>Leverage multi branch pipeline and Jenkinsfile features</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Let&#8217;s us, briefly, go through the tasks we&#8217;ll learn today.</p></div>
<div class="paragraph"><p><strong>Request manual confirmation before deployment:</strong></p></div>
<div class="paragraph"><p>We&#8217;ll define a step in the workflow that will request a manual confirmation before a new release is deployed.</p></div>
<div class="paragraph"><p><strong>Run steps in parallel:</strong></p></div>
<div class="paragraph"><p>We&#8217;ll try to speed up the deployment time by executing steps in parallel.</p></div>
<div class="paragraph"><p><strong>Control the steps execution:</strong></p></div>
<div class="paragraph"><p>In many cases, some conditional logic is required. We&#8217;ll see how we could enhance our Pipeline script by controlling the execution of the steps.</p></div>
<div class="paragraph"><p><strong>Add checkpoints on critical or after long running processes:</strong></p></div>
<div class="paragraph"><p>Even though we should strive for perfection and have systems that never fail, unexpected tends to happen from time to time. We&#8217;ll explore one of the way to fight the unexpected by using checkpoints that will allow us to restart the flow from specified points in the script.</p></div>
<div class="paragraph"><p><strong>Visualize the deployment pipeline:</strong></p></div>
<div class="paragraph"><p>Even though with the Pipeline we are moving towards jobs definitions expressed through code, visualisation is still a very important aspect of the flow. We&#8217;ll see how we could visualise the execution of the pipeline we built.</p></div>
<div class="paragraph"><p><strong>Convert the job into a template:</strong></p></div>
<div class="paragraph"><p>Jenkins jobs tend to be very repetitive. Managing many jobs is very time consuming and error prone endeavour. We&#8217;ll learn how we can improve jobs maintainability by converting the Pipeline job we created into a template.</p></div>
<div class="paragraph"><p><strong>Leverage multi branch pipeline and Jenkinsfile features:</strong></p></div>
<div class="paragraph"><p>Managing pipeline scripts in a centralised location like Jenkins tends to complicate development. Wouldn&#8217;t it be better if the Pipeline code is located in the same repository as the application itself? Moreover, wouldn&#8217;t it be a good improvement if we could define different flows depending on the branch? We&#8217;ll explore both options through the Multibranch Pipeline job and Jenkinsfile.</p></div></aside></section><section id="key_pipeline_dsl_input_task"><h2>Key Pipeline DSL – input: Task</h2><div class="ulist"><ul><li><p>Add an input with the message <em>Please confirm deployment to production</em> and the button with the text <em>Submit</em>.</p></li><li><p>The input should be requested prior to deployment to production.</p></li><li><p>Add the parameter for additional notes and echo the value.</p></li><li><p>Make sure that only the user <em>manager</em> is allowed to use this input.</p></li><li><p>Use user/pass <em>manager/manager</em> to login before approving the deployment.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>The "input" step pauses flow execution and allows the user to interact and control the flow of the build. Only a basic "process" or "abort" option is provided in the stage view. You can optionally request information back, hence the name of the step. The parameter entry screen can be accessed via a link at the bottom of the build console log or via link in the sidebar for a build.</p></div></aside></section><section id="key_pipeline_dsl_input_solution"><h2>Key Pipeline DSL – input: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">    def response = input message: 'Please confirm deployment to production',
        ok: 'Submit',
        parameters: [[
            $class: 'StringParameterDefinition',
            defaultValue: '',
            description: 'Additional comments',
            name: ''
        ]],
        submitter: 'manager'
    echo response</code></pre></div></div></section><section id="key_pipeline_dsl_parallel_task"><h2>Key Pipeline DSL – parallel: Task</h2><div class="paragraph"><p>Modify the script so that <em>service</em> and <em>db</em> containers are pulled in parallel.</p></div>
<aside class="notes"><div class="paragraph"><p>Executing steps in a sequence can result in a long build time. We can reduce it through the parallel step. It allows the execution of multiple flows in parallel.</p></div></aside></section><section id="key_pipeline_dsl_parallel_solution"><h2>Key Pipeline DSL – parallel: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">//    docker.image("localhost:5000/training-books-ms").pull()
//    docker.image("mongo").pull()
    def pull = [:]
    pull["service"] = {
        docker.image("localhost:5000/training-books-ms").pull()
    }
    pull["db"] = {
        docker.image("mongo").pull()
    }
    parallel pull</code></pre></div></div></section><section id="reality_check_4"><h2>Reality Check</h2><div class="ulist"><ul><li><p>Questions on the prep and the Project?</p></li><li><p>Clear on the input and parallel DSL?</p></li><li><p>Ready for the execution control?</p></li></ul></div></section><section id="key_pipeline_dsl_execution_control"><h2>Key Pipeline DSL – execution control</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">retry(10) {
    // some block
}

sleep time: 10, unit: 'MINUTES'

waitUntil {
    // some block
}

timeout(time: 100, unit: 'SECONDS') {
    // some block
}

while(something) {
    // do something
    if (something_else) {
        // do something else
    }
}</code></pre></div></div>
<aside class="notes"><div class="paragraph"><p>In many cases, some conditional logic is required. We&#8217;ll see how we could enhance our Pipeline script by controlling the execution of the steps. There are quite a few execution control steps. We can use retry to repeat a block of steps, wait until an even occurs, pause the execution for a specified period, or timeout if a block of code runs for more than a give period. If those are not enough or do not fulfil the needs, we can always resolve to native Groovy control flow.</p></div></aside></section><section id="key_pipeline_dsl_execution_control_task"><h2>Key Pipeline DSL – execution control: Task</h2><div class="paragraph"><p>Change the script so that the execution of <em>post-deployment tests</em> is retried in case of a failure.</p></div>
<aside class="notes"><div class="paragraph"><p>The "retry" instruction retries the execution of commands in the body up to the specified number of times.</p></div></aside></section><section id="key_pipeline_dsl_execution_control_solution"><h2>Key Pipeline DSL – execution control: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">retry(2) {
    sh "./run_tests.sh"
}</code></pre></div></div></section><section id="key_pipeline_dsl_execution_control_task_2"><h2>Key Pipeline DSL – execution control: Task</h2><div class="paragraph"><p>Change the script so that there is a pause of <em>2 seconds</em> after the service is deployed.</p></div>
<aside class="notes"><div class="paragraph"><p>The "sleep" instruction pauses the execution of the Pipeline.</p></div></aside></section><section id="key_pipeline_dsl_execution_control_solution_2"><h2>Key Pipeline DSL – execution control: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">sleep 2</code></pre></div></div></section><section id="mid_break_4"><h2>Mid-Break</h2><div class="paragraph"><p>(10) minutes for learner re-integration.</p></div>
<div class="imageblock" style=""><div class="content"><img src="./images/break.png" alt="break" width="300"></div></div></section><section id="key_pipeline_dsl_checkpoint_task"><h2>Key Pipeline DSL – checkpoint: Task</h2><div class="paragraph"><p>Add checkpoint <em>deploy</em> between the nodes <em>cd</em> and <em>production</em>.</p></div>
<aside class="notes"><div class="paragraph"><p>Complex pipeline may fail due to external causes (test resource offline, etc…). Restarting from step 1 would be a waste of time and resources. Checkpoints capture the pipeline state so that it can be restarted later.</p></div></aside></section><section id="key_pipeline_dsl_checkpoint_solution"><h2>Key Pipeline DSL – checkpoint: Solution</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">checkpoint "deploy"</code></pre></div></div></section><section id="pipeline_stage_view"><h2>Pipeline Stage View</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-pipeline-stage-view.png" alt="cje pipeline stage view" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>Even though with the Pipeline we are moving towards jobs definitions expressed through code, visualisation is still a very important aspect of the flow. When you have complex builds pipelines, it is useful to be able to see the progress of each stage. To that end, Jenkins Enterprise includes an extended visualization of pipeline build history on the index page of a flow project, under Stage View. (You can also click on Full Stage View to get a full-screen view.)</p></div>
<div class="paragraph"><p>By adding Stage View to your Pipeline jobs it allows users to easily identify the progress of each stage within the job.</p></div></aside></section><section id="pipeline_and_job_template_task"><h2>Pipeline and Job Template: Task</h2><div class="ulist"><ul><li><p>Convert the deployment pipeline script into a template named <em>my-template</em>.</p></li><li><p>Replace the service name (<em>training-books-ms</em>), the registry IP and port (<em>localhost:5000</em>) and domain (<em>http://<strong>&lt;IP&gt;</strong>:8081</em>) with variables.</p></li><li><p>Create a new job called <em>my-pipeline-from-template</em>. The job type should be <em>my-template</em>.</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Jenkins jobs tend to be very repetitive. Managing many jobs is very time consuming and error prone endeavour. We&#8217;ll learn how we can improve jobs maintainability by converting the Pipeline job we created into a template.</p></div></aside></section><section id="pipeline_and_job_template_solution"><h2>Pipeline and Job Template: Solution</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-pipeline-from-template.png" alt="cje pipeline from template" width="700"></div></div></section><section id="multi_branch_pipeline_and_jenkinsfile"><h2>Multi-branch Pipeline and Jenkinsfile</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-multibranch.png" alt="cje multibranch" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>Multi-branch Pipeline job type allows us to use Jenkinsfile as the source of the Pipeline script. Once the job is created, Jenkins will scan the code repository and create sub-projects for each branch. Each of those sub-projects will run the Pipeline script defined in the Jenkinsfile. Having the ability to use a different Jenkinsfile depending on the branch, allows us to fine tune the flow depending on a branch type. For example, the flow defined in Jenkinsfile in the master branch can perform the full delivery all the way until deployment to production. On the other hand, Jenkinsfile inside a feature branch could perform all kinds of testing but skip the deployment itself.</p></div>
<div class="paragraph"><p>Create new job called "my-multibranch". Select "Multibranch Pipeline" as the job type.</p></div></aside></section><section id="multi_branch_pipeline_and_jenkinsfile_2"><h2>Multi-branch Pipeline and Jenkinsfile</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-multibranch-config.png" alt="cje multibranch config" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>Click the "Add Source" button, select "Git" and type "https://github.com/cloudbees/training-books-ms.git" as project repository. Once "Save" button is clicked, Jenkins will scan all branches and create sub-projects for all of them.</p></div></aside></section><section id="multi_branch_pipeline_and_jenkinsfile_3"><h2>Multi-branch Pipeline and Jenkinsfile</h2><div class="listingblock"><div class="content"><pre class="highlight"><code class="groovy language-groovy">def serviceName = "training-books-ms"
def registry = "localhost:5000"
def flow

node("cd") {
    git "https://github.com/cloudbees/${serviceName}.git"
    flow = load "/mnt/scripts/pipeline-common.groovy"
    flow.runPreDeploymentTests(serviceName, registry)
    flow.build(serviceName, registry)
}
checkpoint "deploy"
node("cd") {
    flow.deploy(serviceName, registry)
    flow.runPostDeploymentTests(serviceName, registry, "http://&lt;IP&gt;:8081")
}</code></pre></div></div>
<aside class="notes"><div class="paragraph"><p>This is the content of the Jenkinsfile located in the master branch of the <a href="https://github.com/cloudbees/training-books-ms" class="bare">https://github.com/cloudbees/training-books-ms</a> repository. Functionally, the result is the same as in the Pipeline script we created earlier. However, there is a syntax difference. Unlike previous Pipeline that was defined through a single script, this time we are loading functions from a separate script. This, together with Jenkinsfile placed in code repository, allows developers to define the flow for the own project and keep it in the repository together with the project code. By externalizing common functions inside a separate script we are able to avoid repetition and let developers focus on the flow itself.</p></div></aside></section><section id="multi_branch_pipeline_and_jenkinsfile_4"><h2>Multi-branch Pipeline and Jenkinsfile</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-multibranch-console-security.png" alt="cje multibranch console security" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>When external scripts are used (in this example Jenkinsfile), Pipeline runs with security restrictions. It will fail if it contains unauthorized instructions.</p></div></aside></section><section id="multi_branch_pipeline_and_jenkinsfile_5"><h2>Multi-branch Pipeline and Jenkinsfile</h2><div class="imageblock" style=""><div class="content"><img src="./images/cje-script-approval.png" alt="cje script approval" width="800"></div></div>
<aside class="notes"><div class="paragraph"><p>All unapproved instructions will be listed in the "In-Process Script Approval" screen. Once approved, job can be re-run.</p></div></aside></section><section id="pipeline_global_library"><h2>Pipeline Global Library</h2><div class="ulist"><ul><li><p>Ability to share common parts of Pipeline scripts across multiple jobs</p></li><li><p>Keep scripts DRY</p></li><li><p>Shared library script Git repository</p></li><li><p>Pipeline step <strong>load</strong> vs global library</p></li></ul></div>
<div class="paragraph"><p>Please visit Pipeline Global Library (<a href="https://github.com/jenkinsci/workflow-cps-global-lib-plugin/blob/master/README.md" class="bare">https://github.com/jenkinsci/workflow-cps-global-lib-plugin/blob/master/README.md</a>) for more info.</p></div></section><section id="the_project_part_2_review"><h2>The Project - Part 2: Review</h2></section><section id="the_project_part_2_review_2"><h2>The Project - Part 2: Review</h2><div class="ulist"><ul><li><p>Pipeline Job</p></li><li><p>Job Template</p></li><li><p>Multibranch Pipeline And Jenkinsfile</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Pipeline job:</p></div>
<div class="paragraph"><p><em>WALK THE ATTENDEES THROUGH THE PIPELINE JOB THEY CREATED AND COMMENT ON EACH LINE</em></p></div>
<div class="paragraph"><p>Job template:</p></div>
<div class="paragraph"><p><em>WALK THE ATTENDEES THROUGH THE JOB TEMPLATE THEY CREATED AND COMMENT ON EACH LINE</em></p></div>
<div class="paragraph"><p>Multibranch Pipeline and Jenkinsfile:</p></div>
<div class="paragraph"><p><em>WALK THE ATTENDEES THROUGH THE JOB CONFIGURATION</em>
<em>WALK THE ATTENDEES THROUGH THE JENKINSFILE AND COMMENT ON EACH LINE</em></p></div></aside></section><section id="course_review"><h2>Course Review</h2></section><section id="course_review_2"><h2>Course Review</h2><div class="ulist"><ul><li><p>The Need For The Pipeline</p></li><li><p>What Is Cloudbees Pipeline?</p></li><li><p>The Syntax And The Snippet Generator</p></li><li><p>Docker Containers</p></li><li><p>Docker Tools</p></li><li><p>The Project</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>Pipeline job:</p></div>
<div class="paragraph"><p><em>WALK THE ATTENDEES THROUGH THE PIPELINE JOB THEY CREATED AND COMMENT ON EACH LINE</em></p></div>
<div class="paragraph"><p>Job template:</p></div>
<div class="paragraph"><p><em>WALK THE ATTENDEES THROUGH THE JOB TEMPLATE THEY CREATED AND COMMENT ON EACH LINE</em></p></div>
<div class="paragraph"><p>Multibranch Pipeline and Jenkinsfile:</p></div>
<div class="paragraph"><p><em>WALK THE ATTENDEES THROUGH THE JOB CONFIGURATION</em>
<em>WALK THE ATTENDEES THROUGH THE JENKINSFILE AND COMMENT ON EACH LINE</em></p></div></aside></section><section id="the_project_part_2_exercise"><h2>The Project - Part 2: Exercise</h2><div class="paragraph"><p><a href="labs.html#_the_project_part_2_exercise">The Project - Part 2: Exercise</a></p></div></section><section id="syllabus_and_references"><h2>Syllabus and References</h2><div class="ulist"><ul><li><p>CloudBees Pipeline Plugin Suite (<a href="http://documentation.cloudbees.com/docs/cje-user-guide/workflow.html" class="bare">http://documentation.cloudbees.com/docs/cje-user-guide/workflow.html</a>)</p></li><li><p>CloudBees Docker Pipeline Plugin (<a href="http://documentation.cloudbees.com/docs/cje-user-guide/docker-workflow.html" class="bare">http://documentation.cloudbees.com/docs/cje-user-guide/docker-workflow.html</a>)</p></li><li><p>Docker Docs (<a href="https://docs.docker.com/" class="bare">https://docs.docker.com/</a>)</p></li></ul></div></section></section></div></div><script src="reveal.js/lib/js/head.min.js"></script><script src="reveal.js/js/reveal.js"></script><script>// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display controls in the bottom right corner
  controls: true,
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: true,
  // Push each slide change to the browser history
  history: true,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  previewLinks: false,
  // Theme (e.g., beige, black, league, night, serif, simple, sky, solarized, white)
  // NOTE setting the theme in the config no longer works in reveal.js 3.x
  //theme: Reveal.getQueryHash().theme || 'black',
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: Reveal.getQueryHash().transition || 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1280,
  height: 800,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
  ]
});</script><div id="cloudbees-ruban">
  <img id="cloudbees-logo" src="images/cloudbees_university.svg" alt="CloudBees University Logo"/>

  <span id="presentation-title">CloudBees Jenkins Platform</span>

  <a id="linkToToc" href="#toc"><img src="images/toc-icon.png" alt="ToC Icon" /></a>

  <span id="cloudbees-copyright " class="copyright">© 2016 CloudBees, Inc.  All Rights Reserved</span>
</div>

<div id="watermark">
  <p id="watermark-text">© 2016 CloudBees, Inc.  All Rights Reserved</p>
</div></body></html>
